{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjtUgJJanZK9"
      },
      "source": [
        "# TASK 1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XgZchh0nj7-"
      },
      "source": [
        "# Source Only Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "creeaYfqoO2W"
      },
      "source": [
        "One-time setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT3WWL4xq2nE",
        "outputId": "3b4df634-378b-456e-be3a-6b3dc4bb113a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping apache-beam as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRestarting runtime to finalize PyArrow install...\n"
          ]
        }
      ],
      "source": [
        "# === Fix Arrow/Datasets ABI mismatch (run once) ===\n",
        "%pip -q uninstall -y pyarrow apache-beam\n",
        "%pip -q install -U \"pyarrow>=18.0.0,<19.0.0\" \"datasets>=2.19.0\"\n",
        "\n",
        "# ðŸ” Auto-restart the Colab kernel so the new PyArrow is picked up cleanly\n",
        "import os, IPython\n",
        "print(\"Restarting runtime to finalize PyArrow install...\")\n",
        "IPython.display.clear_output(wait=True)\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GraoafMYnWzk",
        "outputId": "74263c8a-d219-463b-af00-212569381ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cuda\n",
            "Outputs -> /content/drive/MyDrive/DG_PACS/Task1/T1.1_SourceOnly_PACS_photo2sketch_ResNet50\n",
            "HF cache -> /content/drive/MyDrive/hf_cache/datasets\n"
          ]
        }
      ],
      "source": [
        "# === Cell A: Fix deps, auth, mount Drive, config ===\n",
        "# 1) Ensure HF + I/O stack is compatible with your current Colab (transformers/gradio/diffusers)\n",
        "!pip -q uninstall -y huggingface-hub -q\n",
        "!pip -q install -U \"huggingface_hub>=0.34.0,<1.0\" \"datasets>=2.19.0\" \"fsspec==2025.3.0\" \"gcsfs==2025.3.0\"\n",
        "\n",
        "# 2) Colab auth first (prevents \"credential propagation was unsuccessful\")\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# 3) Mount Drive (retry-safe)\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except Exception as e:\n",
        "    # Fallback: unmount then remount once\n",
        "    try:\n",
        "        drive.flush_and_unmount()\n",
        "    except:\n",
        "        pass\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 4) Env + config\n",
        "import os, random, json, time, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Persist HF cache to Drive (no re-downloads later)\n",
        "os.environ[\"HF_HOME\"] = \"/content/drive/MyDrive/hf_cache\"\n",
        "os.environ[\"HF_DATASETS_CACHE\"] = \"/content/drive/MyDrive/hf_cache/datasets\"\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = \"/content/drive/MyDrive/hf_cache/transformers\"\n",
        "\n",
        "def set_all_seeds(seed=1337):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_all_seeds(1337)\n",
        "\n",
        "class Cfg:\n",
        "    # HF PACS domains: 'photo', 'sketch', 'art_painting', 'cartoon'\n",
        "    SOURCE_DOMAIN = \"photo\"\n",
        "    TARGET_DOMAIN = \"sketch\"\n",
        "    NUM_CLASSES = 7\n",
        "    IMG_SIZE = 224\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_WORKERS = 4\n",
        "    EPOCHS = 20\n",
        "    LR = 0.003\n",
        "    WD = 1e-4\n",
        "    MOMENTUM = 0.9\n",
        "    LABEL_SMOOTH = 0.0\n",
        "\n",
        "    # âœ… All Task 1 artifacts -> Drive\n",
        "    SAVE_ROOT = \"/content/drive/MyDrive/DG_PACS/Task1\"\n",
        "    EXP_NAME = \"T1.1_SourceOnly_PACS_photo2sketch_ResNet50\"\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "Cfg = Cfg()\n",
        "\n",
        "def make_out_dirs(exp_name=None):\n",
        "    out_dir = Path(Cfg.SAVE_ROOT) / (exp_name or Cfg.EXP_NAME)\n",
        "    (out_dir / \"figs\").mkdir(parents=True, exist_ok=True)\n",
        "    (out_dir / \"ckpts\").mkdir(parents=True, exist_ok=True)\n",
        "    return out_dir\n",
        "\n",
        "OUT_DIR = make_out_dirs()\n",
        "print(f\"Device: {Cfg.DEVICE}\")\n",
        "print(f\"Outputs -> {OUT_DIR}\")\n",
        "print(\"HF cache ->\", os.environ[\"HF_DATASETS_CACHE\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L7zSlu6otpd"
      },
      "source": [
        "Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNGMQ_1noaN8",
        "outputId": "ee47c8f0-9a44-4295-f7f3-2bc12914313e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Domains: Counter({'sketch': 3929, 'cartoon': 2344, 'art_painting': 2048, 'photo': 1670})\n",
            "Label names: ['dog', 'elephant', 'giraffe', 'guitar', 'horse', 'house', 'person']\n",
            "Classes: 7 -> {0: 'dog', 1: 'elephant', 2: 'giraffe', 3: 'guitar', 4: 'horse', 5: 'house', 6: 'person'}\n",
            "Source: photo | Target: sketch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# === Cell B: PACS via Hugging Face + loaders ===\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# 1) Load PACS (single split with image/domain/label)\n",
        "ds_all = load_dataset(\"flwrlabs/pacs\")  # cached in Drive due to env vars above\n",
        "ds_train = ds_all[\"train\"]\n",
        "\n",
        "# Inspect domains and labels\n",
        "domain_counts = Counter(ds_train[\"domain\"])\n",
        "label_names = ds_train.features[\"label\"].names if hasattr(ds_train.features[\"label\"], \"names\") else None\n",
        "print(\"Domains:\", domain_counts)\n",
        "print(\"Label names:\", label_names)\n",
        "\n",
        "# 2) Transforms\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((Cfg.IMG_SIZE, Cfg.IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.Resize((Cfg.IMG_SIZE, Cfg.IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "# 3) Domain filters\n",
        "def _subset_by_domain(hf_ds, domain_name):\n",
        "    return hf_ds.filter(lambda x: x[\"domain\"] == domain_name)\n",
        "\n",
        "src_hf = _subset_by_domain(ds_train, Cfg.SOURCE_DOMAIN)\n",
        "tgt_hf = _subset_by_domain(ds_train, Cfg.TARGET_DOMAIN)\n",
        "\n",
        "# 4) Torch Dataset wrapper\n",
        "class HFDataset(Dataset):\n",
        "    def __init__(self, hf_split, transform):\n",
        "        self.ds = hf_split\n",
        "        self.transform = transform\n",
        "    def __len__(self): return self.ds.num_rows\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.ds[idx]\n",
        "        img = item[\"image\"].convert(\"RGB\")\n",
        "        y = int(item[\"label\"])\n",
        "        return self.transform(img), y\n",
        "\n",
        "src_train_ds = HFDataset(src_hf, transform=train_tf)\n",
        "src_test_ds  = HFDataset(src_hf, transform=test_tf)   # source \"test\" = same domain, no aug\n",
        "tgt_test_ds  = HFDataset(tgt_hf, transform=test_tf)   # target eval\n",
        "\n",
        "loaders = {\n",
        "    \"src_train\": DataLoader(src_train_ds, batch_size=Cfg.BATCH_SIZE, shuffle=True,\n",
        "                            num_workers=Cfg.NUM_WORKERS, pin_memory=True),\n",
        "    \"src_test\":  DataLoader(src_test_ds,  batch_size=Cfg.BATCH_SIZE, shuffle=False,\n",
        "                            num_workers=Cfg.NUM_WORKERS, pin_memory=True),\n",
        "    \"tgt_test\":  DataLoader(tgt_test_ds,  batch_size=Cfg.BATCH_SIZE, shuffle=False,\n",
        "                            num_workers=Cfg.NUM_WORKERS, pin_memory=True),\n",
        "}\n",
        "\n",
        "IDX2CLASS = {i: name for i, name in enumerate(label_names)} if label_names else {i: f\"class_{i}\" for i in range(Cfg.NUM_CLASSES)}\n",
        "print(f\"Classes: {len(IDX2CLASS)} -> {IDX2CLASS}\")\n",
        "print(f\"Source: {Cfg.SOURCE_DOMAIN} | Target: {Cfg.TARGET_DOMAIN}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhouNJO7oZ0c"
      },
      "source": [
        "Model & training utilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CJCK8Thoind"
      },
      "outputs": [],
      "source": [
        "# === Cell C: Model & utilities ===\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# ---- helpers ----\n",
        "def save_json(obj, path):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps=0.0):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, logits, target):\n",
        "        if self.eps <= 1e-8:\n",
        "            return F.cross_entropy(logits, target)\n",
        "        n = logits.size(1)\n",
        "        log_probs = self.log_softmax(logits)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(log_probs)\n",
        "            true_dist.fill_(self.eps / (n - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.eps)\n",
        "        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n",
        "\n",
        "class ResNet50Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=7, pretrained=True):\n",
        "        super().__init__()\n",
        "        weights = models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        self.backbone = models.resnet50(weights=weights)\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "        all_preds.append(pred.cpu().numpy())\n",
        "        all_labels.append(y.cpu().numpy())\n",
        "    all_preds = np.concatenate(all_preds) if len(all_preds) else np.array([])\n",
        "    all_labels = np.concatenate(all_labels) if len(all_labels) else np.array([])\n",
        "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
        "    return acc, all_preds, all_labels\n",
        "\n",
        "def per_class_stats(preds, labels, idx2class):\n",
        "    if preds.size == 0:\n",
        "        return np.zeros((len(idx2class), len(idx2class)), dtype=int), {idx2class[i]: 0.0 for i in range(len(idx2class))}\n",
        "    cm = confusion_matrix(labels, preds, labels=list(range(len(idx2class))))\n",
        "    per_cls_acc = (cm.diagonal() / cm.sum(axis=1).clip(min=1)) * 100.0\n",
        "    per_cls_dict = {idx2class[i]: float(per_cls_acc[i]) for i in range(len(per_cls_acc))}\n",
        "    return cm, per_cls_dict\n",
        "\n",
        "def plot_confusion(cm, idx2class, title, savepath):\n",
        "    fig = plt.figure(figsize=(6,5))\n",
        "    plt.imshow(cm, interpolation='nearest')\n",
        "    plt.title(title); plt.colorbar()\n",
        "    ticks = np.arange(len(idx2class))\n",
        "    labels = [idx2class[i] for i in range(len(idx2class))]\n",
        "    plt.xticks(ticks, labels, rotation=45, ha='right'); plt.yticks(ticks, labels)\n",
        "    plt.tight_layout(); plt.ylabel('True'); plt.xlabel('Predicted')\n",
        "    plt.savefig(savepath, bbox_inches=\"tight\", dpi=160); plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p5um1eQo5oW"
      },
      "source": [
        "Train Source-Only (ERM on source), then evaluate on source & target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBpgjbtLo6Eo",
        "outputId": "e2146c62-b5c2-4251-c017-4687bd5edd35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 190MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[001/20] loss=1.1498 | src=97.72 | tgt=22.55 | lr=0.00298\n",
            "[002/20] loss=0.1965 | src=99.10 | tgt=16.37 | lr=0.00293\n",
            "[003/20] loss=0.0620 | src=99.82 | tgt=16.90 | lr=0.00284\n",
            "[004/20] loss=0.0376 | src=100.00 | tgt=17.05 | lr=0.00271\n",
            "[005/20] loss=0.0254 | src=100.00 | tgt=16.39 | lr=0.00256\n",
            "[006/20] loss=0.0202 | src=100.00 | tgt=16.26 | lr=0.00238\n",
            "[007/20] loss=0.0176 | src=100.00 | tgt=16.29 | lr=0.00218\n",
            "[008/20] loss=0.0142 | src=100.00 | tgt=16.49 | lr=0.00196\n",
            "[009/20] loss=0.0120 | src=100.00 | tgt=16.70 | lr=0.00173\n",
            "[010/20] loss=0.0105 | src=100.00 | tgt=16.34 | lr=0.00150\n",
            "[011/20] loss=0.0072 | src=100.00 | tgt=16.31 | lr=0.00127\n",
            "[012/20] loss=0.0071 | src=100.00 | tgt=16.29 | lr=0.00104\n",
            "[013/20] loss=0.0077 | src=100.00 | tgt=16.54 | lr=0.00082\n",
            "[014/20] loss=0.0070 | src=100.00 | tgt=16.93 | lr=0.00062\n",
            "[015/20] loss=0.0091 | src=100.00 | tgt=16.34 | lr=0.00044\n",
            "[016/20] loss=0.0066 | src=100.00 | tgt=16.37 | lr=0.00029\n",
            "[017/20] loss=0.0089 | src=100.00 | tgt=16.87 | lr=0.00016\n",
            "[018/20] loss=0.0080 | src=100.00 | tgt=16.42 | lr=0.00007\n",
            "[019/20] loss=0.0059 | src=100.00 | tgt=16.29 | lr=0.00002\n",
            "[020/20] loss=0.0068 | src=100.00 | tgt=16.49 | lr=0.00000\n",
            "Saved: ckpt -> /content/drive/MyDrive/DG_PACS/Task1/T1.1_SourceOnly_PACS_photo2sketch_ResNet50/ckpts/best_by_target.pt | history -> /content/drive/MyDrive/DG_PACS/Task1/T1.1_SourceOnly_PACS_photo2sketch_ResNet50/history.json | curve -> /content/drive/MyDrive/DG_PACS/Task1/T1.1_SourceOnly_PACS_photo2sketch_ResNet50/figs/acc_curve.png\n"
          ]
        }
      ],
      "source": [
        "# === Cell D: Train ERM on source-only & track metrics ===\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch\n",
        "\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "model = ResNet50Classifier(num_classes=Cfg.NUM_CLASSES, pretrained=True).to(device)\n",
        "\n",
        "criterion = LabelSmoothingCE(eps=Cfg.LABEL_SMOOTH)\n",
        "optimizer = SGD(model.parameters(), lr=Cfg.LR, momentum=Cfg.MOMENTUM, weight_decay=Cfg.WD, nesterov=True)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=Cfg.EPOCHS)\n",
        "\n",
        "best_tgt = -1.0\n",
        "history = {\"epoch\": [], \"src_acc\": [], \"tgt_acc\": [], \"lr\": []}\n",
        "\n",
        "for epoch in range(1, Cfg.EPOCHS + 1):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    nseen = 0\n",
        "    for x, y in loaders[\"src_train\"]:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running += loss.item() * y.size(0)\n",
        "        nseen += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # quick eval each epoch\n",
        "    src_acc, _, _ = evaluate(model, loaders[\"src_test\"], device)\n",
        "    tgt_acc, _, _ = evaluate(model, loaders[\"tgt_test\"], device)\n",
        "\n",
        "    history[\"epoch\"].append(epoch)\n",
        "    history[\"src_acc\"].append(src_acc)\n",
        "    history[\"tgt_acc\"].append(tgt_acc)\n",
        "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    print(f\"[{epoch:03d}/{Cfg.EPOCHS}] loss={running/max(nseen,1):.4f} | src={src_acc:.2f} | tgt={tgt_acc:.2f} | lr={history['lr'][-1]:.5f}\")\n",
        "\n",
        "    # save best-by-target checkpoint\n",
        "    if tgt_acc > best_tgt:\n",
        "        best_tgt = tgt_acc\n",
        "        torch.save(model.state_dict(), OUT_DIR / \"ckpts\" / \"best_by_target.pt\")\n",
        "\n",
        "# Save training curves & history to Drive\n",
        "save_json(history, OUT_DIR / \"history.json\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history[\"epoch\"], history[\"src_acc\"], label=\"Source Acc\")\n",
        "plt.plot(history[\"epoch\"], history[\"tgt_acc\"], label=\"Target Acc\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.legend(); plt.title(\"T1.1 Source-Only: Acc vs Epoch\")\n",
        "plt.savefig(OUT_DIR / \"figs\" / \"acc_curve.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "\n",
        "print(f\"Saved: ckpt -> {OUT_DIR/'ckpts'/'best_by_target.pt'} | history -> {OUT_DIR/'history.json'} | curve -> {OUT_DIR/'figs'/'acc_curve.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvTKMP2Ro9qb"
      },
      "source": [
        "Final evaluation, per-class stats, confusion matrices, summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL8kRGkUtPOO",
        "outputId": "80356f2d-e83c-4610-9ff4-284fa4e18816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"source_acc\": 97.72455089820359,\n",
            "  \"target_acc\": 22.55026724357343,\n",
            "  \"avg_domain_acc\": 60.13740907088851,\n",
            "  \"worst_group_acc\": 22.55026724357343\n",
            "}\n",
            "Saved summary and figures to: /content/drive/MyDrive/DG_PACS/Task1/T1.1_SourceOnly_PACS_photo2sketch_ResNet50\n"
          ]
        }
      ],
      "source": [
        "# === Cell E: Final evaluation, per-class stats, confusion matrices, summary (saved to Drive) ===\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 1) Load best checkpoint and eval on source & target\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "model = ResNet50Classifier(num_classes=Cfg.NUM_CLASSES, pretrained=False).to(device)\n",
        "ckpt_path = OUT_DIR / \"ckpts\" / \"best_by_target.pt\"\n",
        "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "\n",
        "src_acc, src_preds, src_labels = evaluate(model, loaders[\"src_test\"], device)\n",
        "tgt_acc, tgt_preds, tgt_labels = evaluate(model, loaders[\"tgt_test\"], device)\n",
        "\n",
        "# 2) Per-class + confusion matrices\n",
        "src_cm, src_percls = per_class_stats(src_preds, src_labels, IDX2CLASS)\n",
        "tgt_cm, tgt_percls = per_class_stats(tgt_preds, tgt_labels, IDX2CLASS)\n",
        "\n",
        "plot_confusion(src_cm, IDX2CLASS, f\"Source ({Cfg.SOURCE_DOMAIN}) Confusion\", OUT_DIR / \"figs\" / \"cm_source.png\")\n",
        "plot_confusion(tgt_cm, IDX2CLASS, f\"Target ({Cfg.TARGET_DOMAIN}) Confusion\", OUT_DIR / \"figs\" / \"cm_target.png\")\n",
        "\n",
        "# 3) Aggregate metrics\n",
        "avg_domain_acc = float((src_acc + tgt_acc) / 2.0)\n",
        "worst_group_acc = float(min(src_acc, tgt_acc))\n",
        "\n",
        "summary = {\n",
        "    \"exp_name\": Cfg.EXP_NAME,\n",
        "    \"domains\": {\"source\": Cfg.SOURCE_DOMAIN, \"target\": Cfg.TARGET_DOMAIN},\n",
        "    \"metrics\": {\n",
        "        \"source_acc\": float(src_acc),\n",
        "        \"target_acc\": float(tgt_acc),\n",
        "        \"avg_domain_acc\": avg_domain_acc,\n",
        "        \"worst_group_acc\": worst_group_acc\n",
        "    },\n",
        "    \"per_class_source\": src_percls,\n",
        "    \"per_class_target\": tgt_percls,\n",
        "    \"artifacts\": {\n",
        "        \"ckpt_best_by_target\": str(ckpt_path),\n",
        "        \"history_json\": str(OUT_DIR / \"history.json\"),\n",
        "        \"acc_curve_png\": str(OUT_DIR / \"figs\" / \"acc_curve.png\"),\n",
        "        \"cm_source_png\": str(OUT_DIR / \"figs\" / \"cm_source.png\"),\n",
        "        \"cm_target_png\": str(OUT_DIR / \"figs\" / \"cm_target.png\"),\n",
        "        \"summary_json\": str(OUT_DIR / \"summary.json\"),\n",
        "    },\n",
        "    \"notes\": \"Source-only baseline (ERM on source). Same backbone will be reused for Task 1 methods.\"\n",
        "}\n",
        "\n",
        "# 4) Save JSON and print a compact recap\n",
        "with open(OUT_DIR / \"summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"source_acc\": summary[\"metrics\"][\"source_acc\"],\n",
        "    \"target_acc\": summary[\"metrics\"][\"target_acc\"],\n",
        "    \"avg_domain_acc\": summary[\"metrics\"][\"avg_domain_acc\"],\n",
        "    \"worst_group_acc\": summary[\"metrics\"][\"worst_group_acc\"],\n",
        "}, indent=2))\n",
        "\n",
        "print(f\"Saved summary and figures to: {OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBa7S5tcrEA1"
      },
      "source": [
        "# Task 1.2: Domain Alignment starting with DANN (adversarial alignment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW8pI-MlsWLy"
      },
      "source": [
        "DANN model pieces (GRL, feature extractor, domain head) + target train loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_kEH0ZNuNgr"
      },
      "source": [
        "Train DANN (source CE + adversarial domain loss) and log curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnZ-8NCouRZw"
      },
      "outputs": [],
      "source": [
        "# === Cell F (memory-lite): DANN utils & tgt-train loader ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- knobs you can tweak quickly ---\n",
        "Cfg.MIXED_PREC = True          # amp on/off\n",
        "Cfg.ACCUM_STEPS = 2            # gradient accumulation to emulate bigger batch\n",
        "Cfg.BATCH_SIZE = max(16, Cfg.BATCH_SIZE // 2)  # halve per-step batch (effective restored via ACCUM_STEPS)\n",
        "Cfg.NUM_WORKERS = 2            # fewer loader workers to save RAM\n",
        "PERSISTENT_WORKERS = False\n",
        "\n",
        "# Rebuild target train loader with new batch size/workers if needed\n",
        "tgt_train_ds = HFDataset(tgt_hf, transform=train_tf)\n",
        "tgt_train_loader = DataLoader(\n",
        "    tgt_train_ds, batch_size=Cfg.BATCH_SIZE, shuffle=True,\n",
        "    num_workers=Cfg.NUM_WORKERS, pin_memory=True, persistent_workers=PERSISTENT_WORKERS\n",
        ")\n",
        "\n",
        "# Feature extractor that can return just features (avoid unnecessary class logits on target)\n",
        "class ResNet_Feature(nn.Module):\n",
        "    def __init__(self, num_classes=7, backbone=\"resnet50\", pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone_name = backbone\n",
        "        if backbone == \"resnet50\":\n",
        "            weights = models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "            net = models.resnet50(weights=weights)\n",
        "            feat_dim = 2048\n",
        "        elif backbone == \"resnet18\":\n",
        "            weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            net = models.resnet18(weights=weights)\n",
        "            feat_dim = 512\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported backbone\")\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            net.conv1, net.bn1, net.relu, net.maxpool,\n",
        "            net.layer1, net.layer2, net.layer3, net.layer4, net.avgpool\n",
        "        )\n",
        "        self.feat_dim = feat_dim\n",
        "        self.classifier = nn.Linear(feat_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, return_feat=False, class_head=True):\n",
        "        f = self.features(x)\n",
        "        f = torch.flatten(f, 1)\n",
        "        if class_head:\n",
        "            logits = self.classifier(f)\n",
        "            return (logits, f) if return_feat else logits\n",
        "        else:\n",
        "            return f  # features only\n",
        "\n",
        "# GRL\n",
        "class GradReverse(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambd): ctx.lambd = lambd; return x.view_as(x)\n",
        "    @staticmethod\n",
        "    def backward(ctx, g): return g.neg() * ctx.lambd, None\n",
        "def grad_reverse(x, lambd=1.0): return GradReverse.apply(x, lambd)\n",
        "\n",
        "# Domain head\n",
        "class DomainDiscriminator(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=1024):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.ReLU(True), nn.Dropout(0.2),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "    def forward(self, feat): return self.net(feat).squeeze(1)\n",
        "\n",
        "# DANN container\n",
        "class DANN(nn.Module):\n",
        "    def __init__(self, num_classes=7, backbone=\"resnet50\", pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = ResNet_Feature(num_classes, backbone=backbone, pretrained=pretrained)\n",
        "        self.domain_disc = DomainDiscriminator(self.backbone.feat_dim)\n",
        "\n",
        "    def forward_domain(self, feat, grl_lambda=1.0):\n",
        "        return self.domain_disc(grad_reverse(feat, grl_lambda))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WYpJ3GP3b8b",
        "outputId": "25103242-844d-4128-ef92-c54833641838"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4227826609.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=Cfg.MIXED_PREC)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4227826609.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=Cfg.MIXED_PREC):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DANN-mem 001/20] src=99.58 | tgt=17.74 | dom_acc=85.9 | Î»=0.243\n",
            "[DANN-mem 002/20] src=99.82 | tgt=33.32 | dom_acc=98.8 | Î»=0.461\n",
            "[DANN-mem 003/20] src=99.82 | tgt=15.50 | dom_acc=91.3 | Î»=0.634\n",
            "[DANN-mem 004/20] src=99.88 | tgt=38.30 | dom_acc=65.7 | Î»=0.761\n",
            "[DANN-mem 005/20] src=99.94 | tgt=49.94 | dom_acc=78.4 | Î»=0.848\n",
            "[DANN-mem 006/20] src=99.76 | tgt=61.03 | dom_acc=74.0 | Î»=0.905\n",
            "[DANN-mem 007/20] src=99.34 | tgt=29.65 | dom_acc=68.5 | Î»=0.941\n",
            "[DANN-mem 008/20] src=99.76 | tgt=28.58 | dom_acc=73.6 | Î»=0.964\n",
            "[DANN-mem 009/20] src=99.64 | tgt=60.91 | dom_acc=66.1 | Î»=0.978\n",
            "[DANN-mem 010/20] src=99.82 | tgt=23.52 | dom_acc=64.1 | Î»=0.987\n",
            "[DANN-mem 011/20] src=100.00 | tgt=61.01 | dom_acc=68.0 | Î»=0.992\n",
            "[DANN-mem 012/20] src=100.00 | tgt=46.12 | dom_acc=58.8 | Î»=0.995\n",
            "[DANN-mem 013/20] src=99.94 | tgt=60.19 | dom_acc=74.2 | Î»=0.997\n",
            "[DANN-mem 014/20] src=99.94 | tgt=54.36 | dom_acc=68.7 | Î»=0.998\n",
            "[DANN-mem 015/20] src=99.94 | tgt=57.60 | dom_acc=67.9 | Î»=0.999\n",
            "[DANN-mem 016/20] src=99.94 | tgt=60.60 | dom_acc=76.0 | Î»=0.999\n",
            "[DANN-mem 017/20] src=99.94 | tgt=61.16 | dom_acc=66.5 | Î»=1.000\n",
            "[DANN-mem 018/20] src=99.88 | tgt=62.23 | dom_acc=67.1 | Î»=1.000\n",
            "[DANN-mem 019/20] src=99.88 | tgt=61.82 | dom_acc=68.0 | Î»=1.000\n",
            "[DANN-mem 020/20] src=99.88 | tgt=62.00 | dom_acc=68.5 | Î»=1.000\n",
            "Saved DANN (mem-lite) artifacts to /content/drive/MyDrive/DG_PACS/Task1/T1.2_DANN_PACS_photo2sketch_ResNet50_memlite\n"
          ]
        }
      ],
      "source": [
        "# === Cell G (memory-lite): Train DANN with concat batches + AMP + grad accumulation ===\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from itertools import cycle\n",
        "import numpy as np, torch, json, matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "model = DANN(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=True).to(device)\n",
        "\n",
        "clf_criterion = LabelSmoothingCE(eps=Cfg.LABEL_SMOOTH)\n",
        "dom_criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = SGD(model.parameters(), lr=Cfg.LR, momentum=Cfg.MOMENTUM, weight_decay=Cfg.WD, nesterov=True)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=Cfg.EPOCHS)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=Cfg.MIXED_PREC)\n",
        "\n",
        "# Î» schedule\n",
        "def dann_lambda(p): return 2.0 / (1.0 + np.exp(-10 * p)) - 1.0\n",
        "\n",
        "# New experiment dir (keeps 1.2 artifacts separate)\n",
        "Cfg.EXP_NAME = \"T1.2_DANN_PACS_photo2sketch_ResNet50_memlite\"\n",
        "OUT_DIR = make_out_dirs()\n",
        "\n",
        "history = {\"epoch\": [], \"src_acc\": [], \"tgt_acc\": [], \"dom_acc\": [], \"dom_loss\": [], \"lambda\": [], \"lr\": []}\n",
        "best_tgt = -1.0\n",
        "\n",
        "src_iter = cycle(loaders[\"src_train\"])\n",
        "tgt_iter = cycle(tgt_train_loader)\n",
        "batches_per_epoch = max(len(loaders[\"src_train\"]), len(tgt_train_loader))\n",
        "\n",
        "accum_steps = max(1, int(Cfg.ACCUM_STEPS))\n",
        "\n",
        "for epoch in range(1, Cfg.EPOCHS+1):\n",
        "    model.train()\n",
        "    dom_correct = 0; dom_total = 0; dom_loss_sum = 0.0\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step in range(batches_per_epoch):\n",
        "        p = ((epoch - 1) * batches_per_epoch + step) / (Cfg.EPOCHS * batches_per_epoch)\n",
        "        lam = float(dann_lambda(p))\n",
        "\n",
        "        xs, ys = next(src_iter)\n",
        "        xt, _  = next(tgt_iter)\n",
        "        # concat once â†’ single forward\n",
        "        x = torch.cat([xs, xt], 0).to(device, non_blocking=True)\n",
        "        ys = ys.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=Cfg.MIXED_PREC):\n",
        "            # forward once; request features; only compute class head for source half\n",
        "            feat_all = model.backbone(x, return_feat=False, class_head=False)  # [B_s+B_t, D]\n",
        "            bs = xs.size(0)\n",
        "            feat_s, feat_t = feat_all[:bs], feat_all[bs:]\n",
        "\n",
        "            # classification on source: run small head on source features only\n",
        "            logits_s = model.backbone.classifier(feat_s)\n",
        "            loss_ce = clf_criterion(logits_s, ys)\n",
        "\n",
        "            # domain loss on source+target (labels: 1 for source, 0 for target)\n",
        "            dom_logits = torch.cat([model.forward_domain(feat_s, lam),\n",
        "                                    model.forward_domain(feat_t, lam)], dim=0)\n",
        "            dom_labels = torch.cat([torch.ones(bs, device=device),\n",
        "                                    torch.zeros(feat_t.size(0), device=device)], dim=0)\n",
        "            loss_dom = dom_criterion(dom_logits, dom_labels)\n",
        "\n",
        "            loss = (loss_ce + loss_dom) / accum_steps  # normalize for accumulation\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % accum_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # quick domain stats (no grad)\n",
        "        with torch.no_grad():\n",
        "            pred_dom = (torch.sigmoid(dom_logits) > 0.5).long()\n",
        "            dom_correct += (pred_dom == dom_labels.long()).sum().item()\n",
        "            dom_total += dom_labels.numel()\n",
        "            dom_loss_sum += float(loss_dom.item())  # already reduced\n",
        "\n",
        "        # free references ASAP\n",
        "        del x, xs, xt, ys, feat_all, feat_s, feat_t, logits_s, dom_logits, dom_labels\n",
        "\n",
        "    # catch leftover grads if batches_per_epoch not divisible by accum_steps\n",
        "    if (batches_per_epoch % accum_steps) != 0:\n",
        "        scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    scheduler.step()\n",
        "    # Light CUDA GC between epochs\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Eval (classification only) using the backbone classifier\n",
        "    src_acc, _, _ = evaluate(model.backbone, loaders[\"src_test\"], device)\n",
        "    tgt_acc, _, _ = evaluate(model.backbone, loaders[\"tgt_test\"], device)\n",
        "\n",
        "    dom_acc = 100.0 * dom_correct / max(dom_total, 1)\n",
        "    history[\"epoch\"].append(epoch)\n",
        "    history[\"src_acc\"].append(float(src_acc))\n",
        "    history[\"tgt_acc\"].append(float(tgt_acc))\n",
        "    history[\"dom_acc\"].append(float(dom_acc))\n",
        "    history[\"dom_loss\"].append(float(dom_loss_sum / max(batches_per_epoch,1)))\n",
        "    history[\"lambda\"].append(lam)\n",
        "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    print(f\"[DANN-mem {epoch:03d}/{Cfg.EPOCHS}] src={src_acc:.2f} | tgt={tgt_acc:.2f} | dom_acc={dom_acc:.1f} | Î»={lam:.3f}\")\n",
        "\n",
        "    if tgt_acc > best_tgt:\n",
        "        best_tgt = tgt_acc\n",
        "        torch.save(model.state_dict(), OUT_DIR / \"ckpts\" / \"best_dann.pt\")\n",
        "        torch.save(model.backbone.state_dict(), OUT_DIR / \"ckpts\" / \"best_dann_backbone.pt\")\n",
        "\n",
        "# Save curves\n",
        "save_json(history, OUT_DIR / \"history_dann.json\")\n",
        "plt.figure(); plt.plot(history[\"epoch\"], history[\"src_acc\"], label=\"Source\"); plt.plot(history[\"epoch\"], history[\"tgt_acc\"], label=\"Target\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Acc (%)\"); plt.legend(); plt.title(\"DANN (mem-lite): Acc\"); plt.savefig(OUT_DIR / \"figs\" / \"dann_acc_curve.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "plt.figure(); plt.plot(history[\"epoch\"], history[\"dom_acc\"], label=\"Domain Acc\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Acc (%)\"); plt.legend(); plt.title(\"DANN (mem-lite): Domain Acc\")\n",
        "plt.savefig(OUT_DIR / \"figs\" / \"dann_domain_acc_curve.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "print(f\"Saved DANN (mem-lite) artifacts to {OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iMUSMXOuW48"
      },
      "source": [
        "Final evaluation + domain-shift proxy (A-distance) + summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppq_C4OSuY0X",
        "outputId": "90934b2a-e4ed-493f-c938-f8abfbb5a222"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3187413579.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
            "/tmp/ipython-input-3187413579.py:68: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"source_acc\": 99.88023952095809,\n",
            "  \"target_acc\": 62.17867141766353,\n",
            "  \"avg_domain_acc\": 81.0294554693108,\n",
            "  \"worst_group_acc\": 62.17867141766353,\n",
            "  \"domain_classifier_acc_eval\": 58.645833333333336,\n",
            "  \"a_distance_hat\": 0.3458333333333332\n",
            "}\n",
            "Saved DANN summary & figures to: /content/drive/MyDrive/DG_PACS/Task1/T1.2_DANN_PACS_photo2sketch_ResNet50_memlite\n"
          ]
        }
      ],
      "source": [
        "# === Cell H (mem-lite): DANN final eval + A-distance proxy + summaries ===\n",
        "import json, numpy as np, torch\n",
        "from itertools import islice\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "\n",
        "# --- rebuild the *mem-lite* DANN you trained ---\n",
        "model = DANN(num_classes=Cfg.NUM_CLASSES, backbone=getattr(Cfg, \"BACKBONE\", \"resnet50\"), pretrained=False).to(device)\n",
        "state = torch.load(OUT_DIR / \"ckpts\" / \"best_dann.pt\", map_location=device)\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "\n",
        "# --- lighter eval loaders (smaller batch, no workers/pin) ---\n",
        "def make_small_loader(big_loader, bs=32):\n",
        "    return DataLoader(\n",
        "        big_loader.dataset, batch_size=min(bs, getattr(Cfg, \"BATCH_SIZE\", 64)),\n",
        "        shuffle=False, num_workers=0, pin_memory=False\n",
        "    )\n",
        "src_test_small = make_small_loader(loaders[\"src_test\"], bs=32)\n",
        "tgt_test_small = make_small_loader(loaders[\"tgt_test\"], bs=32)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_acc_cm(backbone, loader, n_classes):\n",
        "    \"\"\"Streamed accuracy + confusion (no big arrays).\"\"\"\n",
        "    cm = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
        "    total = correct = 0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            # forward with classifier head (backbone is ResNet_Feature)\n",
        "            f = backbone(x, return_feat=False, class_head=False)\n",
        "            logits = backbone.classifier(f)\n",
        "            preds = logits.argmax(1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.numel()\n",
        "        # update confusion\n",
        "        for t, p in zip(y.view(-1).tolist(), preds.view(-1).tolist()):\n",
        "            cm[t, p] += 1\n",
        "    acc = 100.0 * correct / max(total, 1)\n",
        "    # per-class accuracy\n",
        "    percls = {}\n",
        "    for i in range(n_classes):\n",
        "        denom = cm[i].sum()\n",
        "        percls[list(IDX2CLASS.values())[i]] = (100.0 * cm[i, i] / denom) if denom > 0 else 0.0\n",
        "    return acc, cm, percls\n",
        "\n",
        "# --- classification eval (source/target) ---\n",
        "src_acc, src_cm, src_percls = eval_acc_cm(model.backbone, src_test_small, Cfg.NUM_CLASSES)\n",
        "tgt_acc, tgt_cm, tgt_percls = eval_acc_cm(model.backbone, tgt_test_small, Cfg.NUM_CLASSES)\n",
        "\n",
        "# save confusion figures\n",
        "plot_confusion(src_cm, IDX2CLASS, f\"DANN Source ({Cfg.SOURCE_DOMAIN})\", OUT_DIR / \"figs\" / \"dann_cm_source.png\")\n",
        "plot_confusion(tgt_cm, IDX2CLASS, f\"DANN Target ({Cfg.TARGET_DOMAIN})\", OUT_DIR / \"figs\" / \"dann_cm_target.png\")\n",
        "\n",
        "# --- domain proxy (single concatenated forward; capped batches) ---\n",
        "@torch.no_grad()\n",
        "def domain_accuracy_fast(model, src_loader, tgt_loader, max_batches=30):\n",
        "    correct = total = 0\n",
        "    it_src, it_tgt = iter(src_loader), iter(tgt_loader)\n",
        "    for _ in range(max_batches):\n",
        "        try:\n",
        "            xs, _ = next(it_src); xt, _ = next(it_tgt)\n",
        "        except StopIteration:\n",
        "            break\n",
        "        bs = xs.size(0)\n",
        "        x = torch.cat([xs, xt], 0).to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            # one backbone pass for features only\n",
        "            f_all = model.backbone(x, return_feat=False, class_head=False)\n",
        "            f_s, f_t = f_all[:bs], f_all[bs:]\n",
        "            logits = torch.cat([model.forward_domain(f_s, grl_lambda=0.0),\n",
        "                                model.forward_domain(f_t, grl_lambda=0.0)], 0)\n",
        "        labels = torch.cat([torch.ones(bs, device=device),\n",
        "                            torch.zeros(f_t.size(0), device=device)], 0)\n",
        "        pred = (torch.sigmoid(logits) > 0.5).long()\n",
        "        correct += (pred == labels.long()).sum().item()\n",
        "        total += labels.numel()\n",
        "    return 100.0 * correct / max(total, 1)\n",
        "\n",
        "dom_acc_eval = domain_accuracy_fast(model, src_test_small, tgt_test_small, max_batches=30)\n",
        "dom_err = 1.0 - dom_acc_eval / 100.0\n",
        "a_distance_hat = float(2.0 * (1.0 - 2.0 * dom_err))  # 2(1-2Îµ)\n",
        "\n",
        "avg_domain_acc = float((src_acc + tgt_acc) / 2.0)\n",
        "worst_group_acc = float(min(src_acc, tgt_acc))\n",
        "\n",
        "summary = {\n",
        "    \"exp_name\": Cfg.EXP_NAME,\n",
        "    \"domains\": {\"source\": Cfg.SOURCE_DOMAIN, \"target\": Cfg.TARGET_DOMAIN},\n",
        "    \"method\": \"DANN (mem-lite eval)\",\n",
        "    \"metrics\": {\n",
        "        \"source_acc\": float(src_acc),\n",
        "        \"target_acc\": float(tgt_acc),\n",
        "        \"avg_domain_acc\": avg_domain_acc,\n",
        "        \"worst_group_acc\": worst_group_acc,\n",
        "        \"domain_classifier_acc_eval\": float(dom_acc_eval),\n",
        "        \"a_distance_hat\": a_distance_hat\n",
        "    },\n",
        "    \"per_class_source\": src_percls,\n",
        "    \"per_class_target\": tgt_percls,\n",
        "    \"artifacts\": {\n",
        "        \"ckpt_dann\": str(OUT_DIR / \"ckpts\" / \"best_dann.pt\"),\n",
        "        \"ckpt_dann_backbone\": str(OUT_DIR / \"ckpts\" / \"best_dann_backbone.pt\"),\n",
        "        \"history\": str(OUT_DIR / \"history_dann.json\"),\n",
        "        \"acc_curve\": str(OUT_DIR / \"figs\" / \"dann_acc_curve.png\"),\n",
        "        \"domain_acc_curve\": str(OUT_DIR / \"figs\" / \"dann_domain_acc_curve.png\"),\n",
        "        \"cm_source\": str(OUT_DIR / \"figs\" / \"dann_cm_source.png\"),\n",
        "        \"cm_target\": str(OUT_DIR / \"figs\" / \"dann_cm_target.png\"),\n",
        "        \"summary_json\": str(OUT_DIR / \"summary_dann.json\"),\n",
        "    },\n",
        "    \"notes\": \"DANN with GRL; memory-lite eval: small loaders, streamed confusion, single-pass domain proxy.\"\n",
        "}\n",
        "\n",
        "with open(OUT_DIR / \"summary_dann.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(json.dumps(summary[\"metrics\"], indent=2))\n",
        "print(f\"Saved DANN summary & figures to: {OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1jopljAwaTF"
      },
      "source": [
        "DAN (MMD) utilities + training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4jlETnewY5M",
        "outputId": "f61ff29f-a5ac-46fe-a2d6-dc936242ccfe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1408930661.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=getattr(Cfg, \"MIXED_PREC\", True))\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1408930661.py:92: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
            "/tmp/ipython-input-1408930661.py:130: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DAN 001/20] src=14.19 | tgt=15.40 | mmdâ‰ˆ-0.0114\n",
            "[DAN 002/20] src=14.67 | tgt=15.78 | mmdâ‰ˆ-0.0118\n",
            "[DAN 003/20] src=14.55 | tgt=15.30 | mmdâ‰ˆ-0.0115\n",
            "[DAN 004/20] src=14.97 | tgt=15.81 | mmdâ‰ˆ-0.0119\n",
            "[DAN 005/20] src=14.43 | tgt=15.45 | mmdâ‰ˆ-0.0114\n",
            "[DAN 006/20] src=15.39 | tgt=15.68 | mmdâ‰ˆ-0.0117\n",
            "[DAN 007/20] src=15.21 | tgt=15.22 | mmdâ‰ˆ-0.0114\n",
            "[DAN 008/20] src=15.15 | tgt=15.65 | mmdâ‰ˆ-0.0120\n",
            "[DAN 009/20] src=15.51 | tgt=15.93 | mmdâ‰ˆ-0.0119\n",
            "[DAN 010/20] src=15.03 | tgt=15.42 | mmdâ‰ˆ-0.0114\n",
            "[DAN 011/20] src=15.75 | tgt=15.81 | mmdâ‰ˆ-0.0120\n",
            "[DAN 012/20] src=15.51 | tgt=15.30 | mmdâ‰ˆ-0.0115\n",
            "[DAN 013/20] src=15.45 | tgt=15.75 | mmdâ‰ˆ-0.0120\n",
            "[DAN 014/20] src=15.39 | tgt=15.63 | mmdâ‰ˆ-0.0115\n",
            "[DAN 015/20] src=15.93 | tgt=15.73 | mmdâ‰ˆ-0.0120\n",
            "[DAN 016/20] src=16.05 | tgt=15.37 | mmdâ‰ˆ-0.0113\n",
            "[DAN 017/20] src=15.81 | tgt=15.65 | mmdâ‰ˆ-0.0118\n",
            "[DAN 018/20] src=15.87 | tgt=15.91 | mmdâ‰ˆ-0.0118\n",
            "[DAN 019/20] src=15.75 | tgt=15.40 | mmdâ‰ˆ-0.0114\n",
            "[DAN 020/20] src=16.11 | tgt=15.88 | mmdâ‰ˆ-0.0118\n",
            "Saved DAN artifacts to /content/drive/MyDrive/DG_PACS/Task1/T1.3_DAN_PACS_photo2sketch_ResNet50_memlite\n"
          ]
        }
      ],
      "source": [
        "# === Cell I (mem-lite): DAN utils + training ===\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "import numpy as np, json, gc, matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "\n",
        "# ---- Tiny bottleneck + classifier (reuse backbone features) ----\n",
        "class DAN_Head(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes, hid=256, p=0.2):\n",
        "        super().__init__()\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid),\n",
        "            nn.BatchNorm1d(hid),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p),\n",
        "        )\n",
        "        self.classifier = nn.Linear(hid, num_classes)\n",
        "    def forward(self, f, return_feat=False):\n",
        "        z = self.bottleneck(f)\n",
        "        logits = self.classifier(z)\n",
        "        return (logits, z) if return_feat else logits\n",
        "\n",
        "# ---- Unbiased multi-kernel RBF MMD (memory-lite) ----\n",
        "def _pairwise_sq_dists(x, y):\n",
        "    xx = (x @ x.t())\n",
        "    yy = (y @ y.t())\n",
        "    xy = (x @ y.t())\n",
        "    x_sq = xx.diag().unsqueeze(1)\n",
        "    y_sq = yy.diag().unsqueeze(0)\n",
        "    dist_xx = x_sq + x_sq.t() - 2*xx\n",
        "    dist_yy = y_sq + y_sq.t() - 2*yy\n",
        "    dist_xy = x_sq + y_sq - 2*xy\n",
        "    return dist_xx, dist_yy, dist_xy\n",
        "\n",
        "def mmd_unbiased(x, y, sigmas=(2., 5., 10.)):\n",
        "    dist_xx, dist_yy, dist_xy = _pairwise_sq_dists(x, y)\n",
        "    n, m = x.size(0), y.size(0)\n",
        "    Kxx = Kyy = Kxy = 0.0\n",
        "    for s in sigmas:\n",
        "        g = 1.0 / (2.0 * s * s)\n",
        "        Kxx = Kxx + torch.exp(-g * dist_xx)\n",
        "        Kyy = Kyy + torch.exp(-g * dist_yy)\n",
        "        Kxy = Kxy + torch.exp(-g * dist_xy)\n",
        "    Kxx = (Kxx.sum() - Kxx.diag().sum()) / max(n*(n-1), 1)\n",
        "    Kyy = (Kyy.sum() - Kyy.diag().sum()) / max(m*(m-1), 1)\n",
        "    Kxy = Kxy.mean()\n",
        "    return Kxx + Kyy - 2.0 * Kxy\n",
        "\n",
        "# ---- Build model (reuse feature backbone defined earlier) ----\n",
        "Cfg.EXP_NAME = \"T1.3_DAN_PACS_%s2%s_ResNet50_memlite\" % (Cfg.SOURCE_DOMAIN, Cfg.TARGET_DOMAIN)\n",
        "OUT_DIR = make_out_dirs()\n",
        "\n",
        "dan_backbone = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=True).to(device)\n",
        "feat_dim = dan_backbone.feat_dim\n",
        "dan_head = DAN_Head(in_dim=feat_dim, num_classes=Cfg.NUM_CLASSES, hid=256, p=0.2).to(device)\n",
        "\n",
        "# Optim & sched (only train head + backbone; backbone is full but you can optionally freeze more below)\n",
        "for i, p in enumerate(dan_backbone.parameters()):\n",
        "    p.requires_grad = True  # set False for first N params if you want extra savings\n",
        "\n",
        "params = list(dan_backbone.parameters()) + list(dan_head.parameters())\n",
        "optimizer = SGD(params, lr=Cfg.LR, momentum=Cfg.MOMENTUM, weight_decay=Cfg.WD, nesterov=True)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=Cfg.EPOCHS)\n",
        "clf_criterion = LabelSmoothingCE(eps=Cfg.LABEL_SMOOTH)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=getattr(Cfg, \"MIXED_PREC\", True))\n",
        "\n",
        "# Use existing loaders: loaders[\"src_train\"], tgt_train_loader from Cell F\n",
        "src_iter = cycle(loaders[\"src_train\"])\n",
        "tgt_iter = cycle(tgt_train_loader)\n",
        "batches_per_epoch = max(len(loaders[\"src_train\"]), len(tgt_train_loader))\n",
        "accum_steps = max(1, int(getattr(Cfg, \"ACCUM_STEPS\", 1)))\n",
        "\n",
        "history_dan = {\"epoch\": [], \"src_acc\": [], \"tgt_acc\": [], \"mmd\": [], \"lr\": []}\n",
        "best_tgt = -1.0\n",
        "\n",
        "for epoch in range(1, Cfg.EPOCHS+1):\n",
        "    dan_backbone.train(); dan_head.train()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    mmd_running = 0.0\n",
        "    for step in range(batches_per_epoch):\n",
        "        xs, ys = next(src_iter)\n",
        "        xt, _  = next(tgt_iter)\n",
        "        x = torch.cat([xs, xt], 0).to(device, non_blocking=True)\n",
        "        ys = ys.to(device, non_blocking=True)\n",
        "        bs = xs.size(0)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            # one forward for features\n",
        "            f_all = dan_backbone(x, return_feat=False, class_head=False)  # [B_s+B_t, D]\n",
        "            fs, ft = f_all[:bs], f_all[bs:]\n",
        "\n",
        "            # classification only on source\n",
        "            logits_s, zs = dan_head(fs, return_feat=True)\n",
        "            zt = dan_head.bottleneck(ft)  # bottleneck for target (no cls)\n",
        "            loss_ce = clf_criterion(logits_s, ys)\n",
        "            loss_mmd = mmd_unbiased(zs, zt)\n",
        "            loss = (loss_ce + 0.5 * loss_mmd) / accum_steps  # Î»=0.5 by default\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if (step + 1) % accum_steps == 0:\n",
        "            nn.utils.clip_grad_norm_(params, max_norm=5.0)\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        mmd_running += float(loss_mmd.detach())\n",
        "\n",
        "        # free ASAP\n",
        "        del x, xs, xt, ys, f_all, fs, ft, logits_s, zs, zt\n",
        "\n",
        "    if (batches_per_epoch % accum_steps) != 0:\n",
        "        nn.utils.clip_grad_norm_(params, max_norm=5.0)\n",
        "        scaler.step(optimizer); scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    scheduler.step()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    # eval using your small streamed routine from Cell H pattern\n",
        "    @torch.no_grad()\n",
        "    def _eval_acc(backbone, head, loader):\n",
        "        backbone.eval(); head.eval()\n",
        "        total = correct = 0\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast(enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "                f = backbone(x, return_feat=False, class_head=False)\n",
        "                logits = head(f)\n",
        "                pred = logits.argmax(1)\n",
        "            correct += (pred == y).sum().item(); total += y.numel()\n",
        "        return 100.0 * correct / max(total, 1)\n",
        "\n",
        "    src_acc = _eval_acc(dan_backbone, dan_head, loaders[\"src_test\"])\n",
        "    tgt_acc = _eval_acc(dan_backbone, dan_head, loaders[\"tgt_test\"])\n",
        "\n",
        "    history_dan[\"epoch\"].append(epoch)\n",
        "    history_dan[\"src_acc\"].append(float(src_acc))\n",
        "    history_dan[\"tgt_acc\"].append(float(tgt_acc))\n",
        "    history_dan[\"mmd\"].append(float(mmd_running / max(batches_per_epoch,1)))\n",
        "    history_dan[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    print(f\"[DAN {epoch:03d}/{Cfg.EPOCHS}] src={src_acc:.2f} | tgt={tgt_acc:.2f} | mmdâ‰ˆ{history_dan['mmd'][-1]:.4f}\")\n",
        "\n",
        "    if tgt_acc > best_tgt:\n",
        "        best_tgt = tgt_acc\n",
        "        torch.save(dan_backbone.state_dict(), OUT_DIR / \"ckpts\" / \"best_dan_backbone.pt\")\n",
        "        torch.save(dan_head.state_dict(),      OUT_DIR / \"ckpts\" / \"best_dan_head.pt\")\n",
        "\n",
        "# save curves\n",
        "save_json(history_dan, OUT_DIR / \"history_dan.json\")\n",
        "plt.figure(); plt.plot(history_dan[\"epoch\"], history_dan[\"src_acc\"], label=\"Source\"); plt.plot(history_dan[\"epoch\"], history_dan[\"tgt_acc\"], label=\"Target\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Acc (%)\"); plt.legend(); plt.title(\"DAN (mem-lite): Acc\"); plt.savefig(OUT_DIR / \"figs\" / \"dan_acc_curve.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "plt.figure(); plt.plot(history_dan[\"epoch\"], history_dan[\"mmd\"], label=\"MMD\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"DAN: MMD\"); plt.savefig(OUT_DIR / \"figs\" / \"dan_mmd_curve.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "print(f\"Saved DAN artifacts to {OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKvRcIvNwdd7"
      },
      "source": [
        "DAN final evaluation + summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXFfNtX8wgVE",
        "outputId": "05d343ca-6ccd-4521-9309-218b40f24435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"source_acc\": 15.449101796407186,\n",
            "  \"target_acc\": 15.932807330109442,\n",
            "  \"avg_domain_acc\": 15.690954563258314,\n",
            "  \"worst_group_acc\": 15.449101796407186,\n",
            "  \"rare3\": [\n",
            "    5,\n",
            "    6,\n",
            "    3\n",
            "  ],\n",
            "  \"rare3_f1\": 0.07937603542595735,\n",
            "  \"a_distance_hat\": 2.0\n",
            "}\n",
            "Saved DAN summary & figures to: /content/drive/MyDrive/DG_PACS/Task1/T1.3_DAN_PACS_photo2sketch_ResNet50_memlite\n"
          ]
        }
      ],
      "source": [
        "# === Cell J (mem-lite): DAN final eval + rare-class F1 + proxy distance + summary ===\n",
        "import numpy as np, json, torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "\n",
        "# rebuild DAN modules (match Cell I head exactly)\n",
        "dan_backbone = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=False).to(device)\n",
        "dan_backbone.load_state_dict(torch.load(OUT_DIR / \"ckpts\" / \"best_dan_backbone.pt\", map_location=device))\n",
        "\n",
        "class DAN_Head(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes, hid=256, p=0.2):\n",
        "        super().__init__()\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid),\n",
        "            nn.BatchNorm1d(hid),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p),\n",
        "        )\n",
        "        self.classifier = nn.Linear(hid, num_classes)\n",
        "    def forward(self, f):\n",
        "        return self.classifier(self.bottleneck(f))\n",
        "\n",
        "dan_head = DAN_Head(dan_backbone.feat_dim, Cfg.NUM_CLASSES).to(device)\n",
        "dan_head.load_state_dict(torch.load(OUT_DIR / \"ckpts\" / \"best_dan_head.pt\", map_location=device))\n",
        "dan_backbone.eval(); dan_head.eval()\n",
        "\n",
        "# small loaders for eval\n",
        "def make_small_loader(big_loader, bs=32):\n",
        "    return DataLoader(big_loader.dataset, batch_size=min(bs, getattr(Cfg, \"BATCH_SIZE\", 64)),\n",
        "                      shuffle=False, num_workers=0, pin_memory=False)\n",
        "src_small = make_small_loader(loaders[\"src_test\"], bs=32)\n",
        "tgt_small = make_small_loader(loaders[\"tgt_test\"], bs=32)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_acc_cm(backbone, head, loader, idx2class):\n",
        "    cm = np.zeros((len(idx2class), len(idx2class)), dtype=np.int64)\n",
        "    total = correct = 0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)\n",
        "        with torch.amp.autocast('cuda', enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            f = backbone(x, return_feat=False, class_head=False)\n",
        "            logits = head(f); preds = logits.argmax(1)\n",
        "        correct += (preds == y).sum().item(); total += y.numel()\n",
        "        for t, p in zip(y.tolist(), preds.tolist()): cm[t, p] += 1\n",
        "    acc = 100.0 * correct / max(total, 1)\n",
        "    percls = {}\n",
        "    for i in range(len(idx2class)):\n",
        "        denom = cm[i].sum()\n",
        "        percls[idx2class[i]] = (100.0 * cm[i, i] / denom) if denom > 0 else 0.0\n",
        "    return acc, cm, percls\n",
        "\n",
        "src_acc, src_cm, src_percls = eval_acc_cm(dan_backbone, dan_head, src_small, IDX2CLASS)\n",
        "tgt_acc, tgt_cm, tgt_percls = eval_acc_cm(dan_backbone, dan_head, tgt_small, IDX2CLASS)\n",
        "\n",
        "# rare-3 classes (stream counts to stay memory-light)\n",
        "counts = np.zeros(Cfg.NUM_CLASSES, dtype=np.int64)\n",
        "for _, yb in tgt_small:\n",
        "    y_np = yb.numpy() if hasattr(yb, \"numpy\") else np.array(yb)\n",
        "    for t in y_np: counts[t] += 1\n",
        "r3 = np.argsort(counts)[:3].tolist()\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_preds(backbone, head, loader):\n",
        "    ys = []; yh = []\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        with torch.amp.autocast('cuda', enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            f = backbone(x, return_feat=False, class_head=False)\n",
        "            logits = head(f); preds = logits.argmax(1)\n",
        "        ys.append(y.numpy()); yh.append(preds.cpu().numpy())\n",
        "    return np.concatenate(ys), np.concatenate(yh)\n",
        "\n",
        "y_true, y_pred = collect_preds(dan_backbone, dan_head, tgt_small)\n",
        "rare3_f1 = float(f1_score(y_true, y_pred, labels=r3, average=\"macro\"))\n",
        "\n",
        "# ---- proxy domain distance (dtype-safe: ensure FP32) ----\n",
        "@torch.no_grad()\n",
        "def collect_feats(backbone, loader, max_batches=40):\n",
        "    F = []\n",
        "    it = iter(loader)\n",
        "    for _ in range(max_batches):\n",
        "        try: x, _ = next(it)\n",
        "        except StopIteration: break\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        with torch.amp.autocast('cuda', enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            f = backbone(x, return_feat=False, class_head=False)\n",
        "        F.append(f.detach().cpu())\n",
        "    if len(F) == 0:\n",
        "        return torch.zeros(0, dan_backbone.feat_dim, dtype=torch.float32)\n",
        "    return torch.cat(F).float()  # <- force FP32 to avoid Half/Float mismatch\n",
        "\n",
        "Xs = collect_feats(dan_backbone, src_small, max_batches=40)\n",
        "Xt = collect_feats(dan_backbone, tgt_small, max_batches=40)\n",
        "\n",
        "X  = torch.cat([Xs, Xt], 0).to(device).float()  # FP32\n",
        "y  = torch.cat([torch.ones(Xs.size(0)), torch.zeros(Xt.size(0))], 0).to(device).float()\n",
        "\n",
        "lin = nn.Linear(dan_backbone.feat_dim, 1).to(device).float()\n",
        "opt = torch.optim.SGD(lin.parameters(), lr=0.05)\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "lin.train()\n",
        "for _ in range(120):\n",
        "    idx = torch.randperm(X.size(0), device=device)[:256]\n",
        "    xb, yb = X[idx], y[idx]  # already float32\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    # keep probe training in FP32 (no autocast)\n",
        "    loss = bce(lin(xb).squeeze(1), yb)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "lin.eval()\n",
        "with torch.no_grad():\n",
        "    pred = (torch.sigmoid(lin(X).squeeze(1)) > 0.5).float()\n",
        "    dom_err = 1.0 - (pred == y).float().mean().item()\n",
        "a_distance_hat = float(2.0 * (1.0 - 2.0 * dom_err))\n",
        "\n",
        "# save plots\n",
        "plot_confusion(src_cm, IDX2CLASS, f\"DAN Source ({Cfg.SOURCE_DOMAIN})\", OUT_DIR / \"figs\" / \"dan_cm_source.png\")\n",
        "plot_confusion(tgt_cm, IDX2CLASS, f\"DAN Target ({Cfg.TARGET_DOMAIN})\", OUT_DIR / \"figs\" / \"dan_cm_target.png\")\n",
        "\n",
        "avg_domain_acc = float((src_acc + tgt_acc) / 2.0)\n",
        "worst_group_acc = float(min(src_acc, tgt_acc))\n",
        "\n",
        "summary = {\n",
        "    \"exp_name\": Cfg.EXP_NAME,\n",
        "    \"domains\": {\"source\": Cfg.SOURCE_DOMAIN, \"target\": Cfg.TARGET_DOMAIN},\n",
        "    \"method\": \"DAN (mem-lite)\",\n",
        "    \"metrics\": {\n",
        "        \"source_acc\": float(src_acc),\n",
        "        \"target_acc\": float(tgt_acc),\n",
        "        \"avg_domain_acc\": avg_domain_acc,\n",
        "        \"worst_group_acc\": worst_group_acc,\n",
        "        \"rare3\": r3,\n",
        "        \"rare3_f1\": rare3_f1,\n",
        "        \"a_distance_hat\": a_distance_hat\n",
        "    },\n",
        "    \"artifacts\": {\n",
        "        \"ckpt_dan_backbone\": str(OUT_DIR / \"ckpts\" / \"best_dan_backbone.pt\"),\n",
        "        \"ckpt_dan_head\": str(OUT_DIR / \"ckpts\" / \"best_dan_head.pt\"),\n",
        "        \"history\": str(OUT_DIR / \"history_dan.json\"),\n",
        "        \"acc_curve\": str(OUT_DIR / \"figs\" / \"dan_acc_curve.png\"),\n",
        "        \"mmd_curve\": str(OUT_DIR / \"figs\" / \"dan_mmd_curve.png\"),\n",
        "        \"cm_source\": str(OUT_DIR / \"figs\" / \"dan_cm_source.png\"),\n",
        "        \"cm_target\": str(OUT_DIR / \"figs\" / \"dan_cm_target.png\"),\n",
        "        \"summary_json\": str(OUT_DIR / \"summary_dan.json\")\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(OUT_DIR / \"summary_dan.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(json.dumps(summary[\"metrics\"], indent=2))\n",
        "print(f\"Saved DAN summary & figures to: {OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Pp2iJhw4Rl"
      },
      "source": [
        "CDAN utilities + training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbry2GTaw56h",
        "outputId": "9bbb81e5-5d64-40c5-910f-25588fc88cb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 238MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CDAN 001/20] src=14.91 | tgt=15.65 | dom_lossâ‰ˆ0.6945\n",
            "[CDAN 002/20] src=14.73 | tgt=16.09 | dom_lossâ‰ˆ0.6945\n",
            "[CDAN 003/20] src=15.39 | tgt=15.83 | dom_lossâ‰ˆ0.6944\n",
            "[CDAN 004/20] src=14.85 | tgt=15.93 | dom_lossâ‰ˆ0.6944\n",
            "[CDAN 005/20] src=15.63 | tgt=15.96 | dom_lossâ‰ˆ0.6944\n",
            "[CDAN 006/20] src=14.91 | tgt=16.06 | dom_lossâ‰ˆ0.6943\n",
            "[CDAN 007/20] src=15.69 | tgt=16.06 | dom_lossâ‰ˆ0.6942\n",
            "[CDAN 008/20] src=15.09 | tgt=15.78 | dom_lossâ‰ˆ0.6941\n",
            "[CDAN 009/20] src=15.39 | tgt=16.39 | dom_lossâ‰ˆ0.6940\n",
            "[CDAN 010/20] src=15.45 | tgt=15.60 | dom_lossâ‰ˆ0.6939\n",
            "[CDAN 011/20] src=15.15 | tgt=15.93 | dom_lossâ‰ˆ0.6938\n",
            "[CDAN 012/20] src=15.87 | tgt=15.86 | dom_lossâ‰ˆ0.6938\n",
            "[CDAN 013/20] src=15.09 | tgt=16.14 | dom_lossâ‰ˆ0.6937\n",
            "[CDAN 014/20] src=16.11 | tgt=15.88 | dom_lossâ‰ˆ0.6936\n",
            "[CDAN 015/20] src=15.15 | tgt=16.01 | dom_lossâ‰ˆ0.6935\n",
            "[CDAN 016/20] src=15.75 | tgt=16.01 | dom_lossâ‰ˆ0.6934\n",
            "[CDAN 017/20] src=15.45 | tgt=15.65 | dom_lossâ‰ˆ0.6934\n",
            "[CDAN 018/20] src=15.63 | tgt=16.47 | dom_lossâ‰ˆ0.6933\n",
            "[CDAN 019/20] src=15.87 | tgt=15.60 | dom_lossâ‰ˆ0.6933\n",
            "[CDAN 020/20] src=15.33 | tgt=16.06 | dom_lossâ‰ˆ0.6932\n",
            "Saved CDAN artifacts to /content/drive/MyDrive/DG_PACS/Task1/T1.4_CDAN_PACS_photo2sketch_ResNet50_memlite\n"
          ]
        }
      ],
      "source": [
        "# === Cell K (mem-lite): CDAN utils + training â€” FIXED ===\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "import numpy as np, json, gc, matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "\n",
        "class CDAN_Head(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes, hid=256, p=0.2):\n",
        "        super().__init__()\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid),\n",
        "            nn.BatchNorm1d(hid),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p),\n",
        "        )\n",
        "        self.classifier = nn.Linear(hid, num_classes)\n",
        "    def forward(self, f, return_feat=False, return_prob=False):\n",
        "        z = self.bottleneck(f)\n",
        "        logits = self.classifier(z)\n",
        "        if return_prob:\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            return logits, z, probs\n",
        "        return (logits, z) if return_feat else logits\n",
        "\n",
        "class Compress(nn.Module):\n",
        "    def __init__(self, feat_dim, num_classes, out_dim=512):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(feat_dim * num_classes, out_dim)\n",
        "    def forward(self, z, p):\n",
        "        op = torch.bmm(p.unsqueeze(2), z.unsqueeze(1))  # [B,C,F]\n",
        "        op = op.view(op.size(0), -1)                    # [B, C*F]\n",
        "        return self.lin(op)\n",
        "\n",
        "class SmallDomainDisc(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256), nn.ReLU(True),\n",
        "            nn.Linear(256, 128), nn.ReLU(True),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)  # -> [B]\n",
        "\n",
        "def grl(x, lambd=1.0): return grad_reverse(x, lambd)\n",
        "\n",
        "Cfg.EXP_NAME = f\"T1.4_CDAN_PACS_{Cfg.SOURCE_DOMAIN}2{Cfg.TARGET_DOMAIN}_ResNet50_memlite\"\n",
        "OUT_DIR = make_out_dirs()\n",
        "\n",
        "cdan_backbone = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=True).to(device)\n",
        "feat_dim = cdan_backbone.feat_dim\n",
        "cdan_head = CDAN_Head(feat_dim, Cfg.NUM_CLASSES, hid=256, p=0.2).to(device)\n",
        "compress  = Compress(256, Cfg.NUM_CLASSES, out_dim=512).to(device)\n",
        "d_disc    = SmallDomainDisc(512).to(device)\n",
        "\n",
        "params_main = list(cdan_backbone.parameters()) + list(cdan_head.parameters()) + list(compress.parameters())\n",
        "opt_main = SGD(params_main, lr=Cfg.LR, momentum=Cfg.MOMENTUM, weight_decay=Cfg.WD, nesterov=True)\n",
        "opt_d    = SGD(d_disc.parameters(), lr=Cfg.LR, momentum=Cfg.MOMENTUM, weight_decay=Cfg.WD)\n",
        "sched_main = CosineAnnealingLR(opt_main, T_max=Cfg.EPOCHS)\n",
        "\n",
        "clf_criterion = LabelSmoothingCE(eps=Cfg.LABEL_SMOOTH)\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# âœ… New AMP scaler API\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=getattr(Cfg, \"MIXED_PREC\", True))\n",
        "\n",
        "src_iter = cycle(loaders[\"src_train\"])\n",
        "tgt_iter = cycle(tgt_train_loader)\n",
        "batches_per_epoch = max(len(loaders[\"src_train\"]), len(tgt_train_loader))\n",
        "accum_steps = max(1, int(getattr(Cfg, \"ACCUM_STEPS\", 1)))\n",
        "\n",
        "def entropy_weight(p):\n",
        "    ent = -(p.clamp_min(1e-6) * p.clamp_min(1e-6).log()).sum(1)\n",
        "    ent = ent / np.log(Cfg.NUM_CLASSES)\n",
        "    return (1.0 + torch.exp(-ent)).detach()\n",
        "\n",
        "history_cdan = {\"epoch\": [], \"src_acc\": [], \"tgt_acc\": [], \"dom_loss\": [], \"lr\": []}\n",
        "best_tgt = -1.0\n",
        "\n",
        "for epoch in range(1, Cfg.EPOCHS+1):\n",
        "    cdan_backbone.train(); cdan_head.train(); compress.train(); d_disc.train()\n",
        "    opt_main.zero_grad(set_to_none=True); opt_d.zero_grad(set_to_none=True)\n",
        "\n",
        "    dom_loss_sum = 0.0\n",
        "\n",
        "    for step in range(batches_per_epoch):\n",
        "        xs, ys = next(src_iter)\n",
        "        xt, _  = next(tgt_iter)\n",
        "        x = torch.cat([xs, xt], 0).to(device, non_blocking=True)\n",
        "        ys = ys.to(device, non_blocking=True)\n",
        "        bs = xs.size(0)\n",
        "\n",
        "        # ---- main (cls + adv via GRL) ----\n",
        "        with torch.amp.autocast('cuda', enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            f_all = cdan_backbone(x, return_feat=False, class_head=False)\n",
        "            fs, ft = f_all[:bs], f_all[bs:]\n",
        "\n",
        "            logits_s, zs, ps = cdan_head(fs, return_feat=True, return_prob=True)\n",
        "            loss_ce = clf_criterion(logits_s, ys)\n",
        "\n",
        "            _, zt, pt = cdan_head(ft, return_feat=True, return_prob=True)\n",
        "\n",
        "            cond_s = compress(zs, ps)\n",
        "            cond_t = compress(zt, pt)\n",
        "            cond   = torch.cat([cond_s, cond_t], 0)\n",
        "\n",
        "            dom_logits = d_disc(grl(cond, 1.0))  # already [B]; no .squeeze(1)\n",
        "            dom_labels = torch.cat([torch.ones(bs, device=device),\n",
        "                                    torch.zeros(cond_t.size(0), device=device)], 0)\n",
        "            w = torch.cat([entropy_weight(ps), entropy_weight(pt)], 0)\n",
        "            loss_adv = (bce(dom_logits, dom_labels) * w).mean()\n",
        "\n",
        "            loss = (loss_ce + 0.5 * loss_adv) / accum_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if (step + 1) % accum_steps == 0:\n",
        "            nn.utils.clip_grad_norm_(params_main, max_norm=5.0)\n",
        "            scaler.step(opt_main); scaler.update()\n",
        "            opt_main.zero_grad(set_to_none=True)\n",
        "\n",
        "        # ---- update D on detached cond ----\n",
        "        with torch.no_grad():\n",
        "            cond_s_d = cond_s.detach(); cond_t_d = cond_t.detach()\n",
        "            dom_in = torch.cat([cond_s_d, cond_t_d], 0)\n",
        "            dom_y  = torch.cat([torch.ones(bs, device=device),\n",
        "                                torch.zeros(cond_t_d.size(0), device=device)], 0)\n",
        "\n",
        "        opt_d.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast('cuda', enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            dom_logits_d = d_disc(dom_in)         # shape [B]; âœ… no .squeeze(1)\n",
        "            loss_d = bce(dom_logits_d, dom_y)     # dom_y is [B]\n",
        "        scaler.scale(loss_d).backward()\n",
        "        nn.utils.clip_grad_norm_(d_disc.parameters(), max_norm=5.0)\n",
        "        scaler.step(opt_d); scaler.update()\n",
        "\n",
        "        dom_loss_sum += float(loss_d.detach())\n",
        "\n",
        "        del x, xs, xt, ys, f_all, fs, ft, logits_s, zs, ps, zt, pt, cond_s, cond_t, cond, dom_logits, dom_labels, w, dom_logits_d, dom_y\n",
        "\n",
        "    sched_main.step()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _eval_acc(backbone, head, loader):\n",
        "        backbone.eval(); head.eval()\n",
        "        total = correct = 0\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)\n",
        "            with torch.amp.autocast('cuda', enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "                f = backbone(x, return_feat=False, class_head=False)\n",
        "                logits = head(f)\n",
        "                pred = logits.argmax(1)\n",
        "            correct += (pred == y).sum().item(); total += y.numel()\n",
        "        return 100.0 * correct / max(total, 1)\n",
        "\n",
        "    src_acc = _eval_acc(cdan_backbone, cdan_head, loaders[\"src_test\"])\n",
        "    tgt_acc = _eval_acc(cdan_backbone, cdan_head, loaders[\"tgt_test\"])\n",
        "\n",
        "    history_cdan[\"epoch\"].append(epoch)\n",
        "    history_cdan[\"src_acc\"].append(float(src_acc))\n",
        "    history_cdan[\"tgt_acc\"].append(float(tgt_acc))\n",
        "    history_cdan[\"dom_loss\"].append(float(dom_loss_sum / max(batches_per_epoch,1)))\n",
        "    history_cdan[\"lr\"].append(opt_main.param_groups[0][\"lr\"])\n",
        "\n",
        "    print(f\"[CDAN {epoch:03d}/{Cfg.EPOCHS}] src={src_acc:.2f} | tgt={tgt_acc:.2f} | dom_lossâ‰ˆ{history_cdan['dom_loss'][-1]:.4f}\")\n",
        "\n",
        "    if tgt_acc > best_tgt:\n",
        "        best_tgt = tgt_acc\n",
        "        torch.save(cdan_backbone.state_dict(), OUT_DIR / \"ckpts\" / \"best_cdan_backbone.pt\")\n",
        "        torch.save(cdan_head.state_dict(),      OUT_DIR / \"ckpts\" / \"best_cdan_head.pt\")\n",
        "        torch.save(compress.state_dict(),       OUT_DIR / \"ckpts\" / \"best_cdan_compress.pt\")\n",
        "        torch.save(d_disc.state_dict(),         OUT_DIR / \"ckpts\" / \"best_cdan_ddisc.pt\")\n",
        "\n",
        "save_json(history_cdan, OUT_DIR / \"history_cdan.json\")\n",
        "plt.figure(); plt.plot(history_cdan[\"epoch\"], history_cdan[\"src_acc\"], label=\"Source\"); plt.plot(history_cdan[\"epoch\"], history_cdan[\"tgt_acc\"], label=\"Target\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Acc (%)\"); plt.legend(); plt.title(\"CDAN (mem-lite): Acc\")\n",
        "plt.savefig(OUT_DIR / \"figs\" / \"cdan_acc_curve.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "\n",
        "plt.figure(); plt.plot(history_cdan[\"epoch\"], history_cdan[\"dom_loss\"], label=\"Domain Loss\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"CDAN: Domain Loss\")\n",
        "plt.savefig(OUT_DIR / \"figs\" / \"cdan_dom_loss_curve.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "\n",
        "print(f\"Saved CDAN artifacts to {OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Yq9obRHw71q"
      },
      "source": [
        "CDAN final evaluation + A-distance proxy + summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAVDWoWLw-Yd"
      },
      "outputs": [],
      "source": [
        "# === Cell L (mem-lite): CDAN final eval + rare-class F1 + proxy distance + summary ===\n",
        "import numpy as np, json, torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "\n",
        "# --- rebuild CDAN modules EXACTLY like Cell K ---\n",
        "class CDAN_Head(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes, hid=256, p=0.2):\n",
        "        super().__init__()\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid),\n",
        "            nn.BatchNorm1d(hid),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p),\n",
        "        )\n",
        "        self.classifier = nn.Linear(hid, num_classes)\n",
        "    def forward(self, f):\n",
        "        z = self.bottleneck(f)\n",
        "        return self.classifier(z)\n",
        "\n",
        "cdan_backbone = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=False).to(device)\n",
        "cdan_backbone.load_state_dict(torch.load(OUT_DIR / \"ckpts\" / \"best_cdan_backbone.pt\", map_location=device))\n",
        "cdan_head = CDAN_Head(cdan_backbone.feat_dim, Cfg.NUM_CLASSES).to(device)\n",
        "cdan_head.load_state_dict(torch.load(OUT_DIR / \"ckpts\" / \"best_cdan_head.pt\", map_location=device))\n",
        "cdan_backbone.eval(); cdan_head.eval()\n",
        "\n",
        "# --- small loaders for eval ---\n",
        "def make_small_loader(big_loader, bs=32):\n",
        "    return DataLoader(big_loader.dataset, batch_size=min(bs, getattr(Cfg, \"BATCH_SIZE\", 64)),\n",
        "                      shuffle=False, num_workers=0, pin_memory=False)\n",
        "src_small = make_small_loader(loaders[\"src_test\"], bs=32)\n",
        "tgt_small = make_small_loader(loaders[\"tgt_test\"], bs=32)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_acc_cm(backbone, head, loader, idx2class):\n",
        "    cm = np.zeros((len(idx2class), len(idx2class)), dtype=np.int64)\n",
        "    total = correct = 0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)\n",
        "        with torch.amp.autocast('cuda', enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            f = backbone(x, return_feat=False, class_head=False)\n",
        "            logits = head(f); preds = logits.argmax(1)\n",
        "        correct += (preds == y).sum().item(); total += y.numel()\n",
        "        for t, p in zip(y.tolist(), preds.tolist()):\n",
        "            cm[t, p] += 1\n",
        "    acc = 100.0 * correct / max(total, 1)\n",
        "    percls = {}\n",
        "    for i in range(len(idx2class)):\n",
        "        denom = cm[i].sum()\n",
        "        percls[idx2class[i]] = (100.0 * cm[i, i] / denom) if denom > 0 else 0.0\n",
        "    return acc, cm, percls\n",
        "\n",
        "# --- classification eval ---\n",
        "src_acc, src_cm, src_percls = eval_acc_cm(cdan_backbone, cdan_head, src_small, IDX2CLASS)\n",
        "tgt_acc, tgt_cm, tgt_percls = eval_acc_cm(cdan_backbone, cdan_head, tgt_small, IDX2CLASS)\n",
        "\n",
        "# --- rare-3 F1 (stream counts) ---\n",
        "counts = np.zeros(Cfg.NUM_CLASSES, dtype=np.int64)\n",
        "for _, yb in tgt_small:\n",
        "    y_np = yb.numpy() if hasattr(yb, \"numpy\") else np.array(yb)\n",
        "    for t in y_np: counts[t] += 1\n",
        "r3 = np.argsort(counts)[:3].tolist()\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_preds(backbone, head, loader):\n",
        "    ys = []; yh = []\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        with torch.amp.autocast('cuda', enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            f = backbone(x, return_feat=False, class_head=False)\n",
        "            logits = head(f); preds = logits.argmax(1)\n",
        "        ys.append(y.numpy()); yh.append(preds.cpu().numpy())\n",
        "    return np.concatenate(ys), np.concatenate(yh)\n",
        "\n",
        "y_true, y_pred = collect_preds(cdan_backbone, cdan_head, tgt_small)\n",
        "rare3_f1 = float(f1_score(y_true, y_pred, labels=r3, average=\"macro\"))\n",
        "\n",
        "# --- proxy domain distance (dtype-safe FP32) ---\n",
        "@torch.no_grad()\n",
        "def collect_feats(backbone, loader, max_batches=40):\n",
        "    F = []\n",
        "    it = iter(loader)\n",
        "    for _ in range(max_batches):\n",
        "        try: x, _ = next(it)\n",
        "        except StopIteration: break\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        with torch.amp.autocast('cuda', enabled=getattr(Cfg, \"MIXED_PREC\", True)):\n",
        "            f = backbone(x, return_feat=False, class_head=False)\n",
        "        F.append(f.detach().cpu())\n",
        "    if len(F) == 0:\n",
        "        return torch.zeros(0, cdan_backbone.feat_dim, dtype=torch.float32)\n",
        "    return torch.cat(F).float()  # ensure FP32\n",
        "\n",
        "Xs = collect_feats(cdan_backbone, src_small, max_batches=40)\n",
        "Xt = collect_feats(cdan_backbone, tgt_small, max_batches=40)\n",
        "X  = torch.cat([Xs, Xt], 0).to(device).float()\n",
        "y  = torch.cat([torch.ones(Xs.size(0)), torch.zeros(Xt.size(0))], 0).to(device).float()\n",
        "\n",
        "lin = nn.Linear(cdan_backbone.feat_dim, 1).to(device).float()\n",
        "opt = torch.optim.SGD(lin.parameters(), lr=0.05)\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "lin.train()\n",
        "for _ in range(120):\n",
        "    idx = torch.randperm(X.size(0), device=device)[:256]\n",
        "    xb, yb = X[idx], y[idx]\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    # FP32 probe (no autocast)\n",
        "    loss = bce(lin(xb).squeeze(1), yb)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "lin.eval()\n",
        "with torch.no_grad():\n",
        "    pred = (torch.sigmoid(lin(X).squeeze(1)) > 0.5).float()\n",
        "    dom_err = 1.0 - (pred == y).float().mean().item()\n",
        "a_distance_hat = float(2.0 * (1.0 - 2.0 * dom_err))\n",
        "\n",
        "# --- save plots ---\n",
        "plot_confusion(src_cm, IDX2CLASS, f\"CDAN Source ({Cfg.SOURCE_DOMAIN})\", OUT_DIR / \"figs\" / \"cdan_cm_source.png\")\n",
        "plot_confusion(tgt_cm, IDX2CLASS, f\"CDAN Target ({Cfg.TARGET_DOMAIN})\", OUT_DIR / \"figs\" / \"cdan_cm_target.png\")\n",
        "\n",
        "avg_domain_acc = float((src_acc + tgt_acc) / 2.0)\n",
        "worst_group_acc = float(min(src_acc, tgt_acc))\n",
        "\n",
        "summary = {\n",
        "    \"exp_name\": Cfg.EXP_NAME,\n",
        "    \"domains\": {\"source\": Cfg.SOURCE_DOMAIN, \"target\": Cfg.TARGET_DOMAIN},\n",
        "    \"method\": \"CDAN (mem-lite)\",\n",
        "    \"metrics\": {\n",
        "        \"source_acc\": float(src_acc),\n",
        "        \"target_acc\": float(tgt_acc),\n",
        "        \"avg_domain_acc\": avg_domain_acc,\n",
        "        \"worst_group_acc\": worst_group_acc,\n",
        "        \"rare3\": r3,\n",
        "        \"rare3_f1\": rare3_f1,\n",
        "        \"a_distance_hat\": a_distance_hat\n",
        "    },\n",
        "    \"artifacts\": {\n",
        "        \"ckpt_cdan_backbone\": str(OUT_DIR / \"ckpts\" / \"best_cdan_backbone.pt\"),\n",
        "        \"ckpt_cdan_head\": str(OUT_DIR / \"ckpts\" / \"best_cdan_head.pt\"),\n",
        "        \"history\": str(OUT_DIR / \"history_cdan.json\"),\n",
        "        \"acc_curve\": str(OUT_DIR / \"figs\" / \"cdan_acc_curve.png\"),\n",
        "        \"dom_loss_curve\": str(OUT_DIR / \"figs\" / \"cdan_dom_loss_curve.png\"),\n",
        "        \"cm_source\": str(OUT_DIR / \"figs\" / \"cdan_cm_source.png\"),\n",
        "        \"cm_target\": str(OUT_DIR / \"figs\" / \"cdan_cm_target.png\"),\n",
        "        \"summary_json\": str(OUT_DIR / \"summary_cdan.json\")\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(OUT_DIR / \"summary_cdan.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(json.dumps(summary[\"metrics\"], indent=2))\n",
        "print(f\"Saved CDAN summary & figures to: {OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell M: Build-eval helpers + load all Task-1 checkpoints ===\n",
        "import json, torch, numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "\n",
        "SAVE_ROOT = Path(Cfg.SAVE_ROOT)\n",
        "\n",
        "# Known exp names from your run:\n",
        "EXP_ERM  = \"T1.1_SourceOnly_PACS_photo2sketch_ResNet50\"\n",
        "EXP_DANN = \"T1.2_DANN_PACS_photo2sketch_ResNet50_memlite\"\n",
        "EXP_DAN  = \"T1.3_DAN_PACS_photo2sketch_ResNet50_memlite\"\n",
        "EXP_CDAN = \"T1.4_CDAN_PACS_photo2sketch_ResNet50_memlite\"\n",
        "\n",
        "# --- Minimal feature backbones matching your code ---\n",
        "class ResNet50Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=Cfg.NUM_CLASSES, pretrained=False):\n",
        "        super().__init__()\n",
        "        from torchvision import models\n",
        "        weights = models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        self.backbone = models.resnet50(weights=weights)\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
        "    def forward(self, x): return self.backbone(x)\n",
        "\n",
        "# From your DANN/DAN/CDAN code (assumed available in session):\n",
        "# - DANN(backbone=\"resnet50\") with .backbone and .forward_domain\n",
        "# - ResNet_Feature(backbone=\"resnet50\") exposing .feat_dim and returning feats when requested\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_clf(model, loader):\n",
        "    model.eval()\n",
        "    total = correct = 0\n",
        "    for x,y in loader:\n",
        "        x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred==y).sum().item(); total += y.numel()\n",
        "    return 100.0 * correct / max(total,1)\n",
        "\n",
        "def make_small_loader(big_loader, bs=32):\n",
        "    return DataLoader(big_loader.dataset, batch_size=min(bs, Cfg.BATCH_SIZE),\n",
        "                      shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "small_src = make_small_loader(loaders[\"src_test\"], 32)\n",
        "small_tgt = make_small_loader(loaders[\"tgt_test\"], 32)\n",
        "\n",
        "# --- Rebuilders that return a callable \"classifier(x)->logits\" for uniform eval ---\n",
        "def build_erm():\n",
        "    path = SAVE_ROOT/EXP_ERM/\"ckpts\"/\"best_by_target.pt\"\n",
        "    m = ResNet50Classifier(num_classes=Cfg.NUM_CLASSES, pretrained=False).to(device)\n",
        "    m.load_state_dict(torch.load(path, map_location=device)); m.eval()\n",
        "    return m\n",
        "\n",
        "def build_dann_backbone():\n",
        "    path = SAVE_ROOT/EXP_DANN/\"ckpts\"/\"best_dann_backbone.pt\"\n",
        "    b = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=False).to(device)\n",
        "    b.load_state_dict(torch.load(path, map_location=device)); b.eval()\n",
        "    return b\n",
        "\n",
        "class DAN_Head(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes, hid=256, p=0.2):\n",
        "        super().__init__()\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid), nn.BatchNorm1d(hid), nn.ReLU(True), nn.Dropout(p)\n",
        "        )\n",
        "        self.classifier = nn.Linear(hid, num_classes)\n",
        "    def forward(self, f): return self.classifier(self.bottleneck(f))\n",
        "\n",
        "def build_dan():\n",
        "    b = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=False).to(device)\n",
        "    h = DAN_Head(b.feat_dim, Cfg.NUM_CLASSES).to(device)\n",
        "    b.load_state_dict(torch.load(SAVE_ROOT/EXP_DAN/\"ckpts\"/\"best_dan_backbone.pt\", map_location=device))\n",
        "    h.load_state_dict(torch.load(SAVE_ROOT/EXP_DAN/\"ckpts\"/\"best_dan_head.pt\", map_location=device))\n",
        "    b.eval(); h.eval()\n",
        "    class C(nn.Module):\n",
        "        def __init__(self,b,h): super().__init__(); self.b,self.h=b,h\n",
        "        def forward(self,x):\n",
        "            f = self.b(x, return_feat=False, class_head=False)\n",
        "            return self.h(f)\n",
        "    return C(b,h).to(device)\n",
        "\n",
        "class CDAN_Head(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes, hid=256, p=0.2):\n",
        "        super().__init__()\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid), nn.BatchNorm1d(hid), nn.ReLU(True), nn.Dropout(p)\n",
        "        )\n",
        "        self.classifier = nn.Linear(hid, num_classes)\n",
        "    def forward(self, f): return self.classifier(self.bottleneck(f))\n",
        "\n",
        "def build_cdan():\n",
        "    b = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=False).to(device)\n",
        "    h = CDAN_Head(b.feat_dim, Cfg.NUM_CLASSES).to(device)\n",
        "    b.load_state_dict(torch.load(SAVE_ROOT/EXP_CDAN/\"ckpts\"/\"best_cdan_backbone.pt\", map_location=device))\n",
        "    h.load_state_dict(torch.load(SAVE_ROOT/EXP_CDAN/\"ckpts\"/\"best_cdan_head.pt\", map_location=device))\n",
        "    b.eval(); h.eval()\n",
        "    class C(nn.Module):\n",
        "        def __init__(self,b,h): super().__init__(); self.b,self.h=b,h\n",
        "        def forward(self,x):\n",
        "            f = self.b(x, return_feat=False, class_head=False)\n",
        "            return self.h(f)\n",
        "    return C(b,h).to(device)\n",
        "\n",
        "MODELS = {\n",
        "    \"ERM\": build_erm,\n",
        "    \"DANN\": lambda: nn.Sequential(build_dann_backbone(), nn.Identity()),  # eval via .backbone classifier below in your H cell\n",
        "    \"DAN\": build_dan,\n",
        "    \"CDAN\": build_cdan,\n",
        "}\n",
        "\n",
        "print(\"Helpers ready. You can now build/eval: ERM, DAN, CDAN. (DANN uses backbone+classifier as in your H cell.)\")\n"
      ],
      "metadata": {
        "id": "VYr_Q5I95Wh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5e2e54-51bc-4d70-cb44-35c578ea80b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helpers ready. You can now build/eval: ERM, DAN, CDAN. (DANN uses backbone+classifier as in your H cell.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Training on Target (pseudo-labels) â€” memory-lite"
      ],
      "metadata": {
        "id": "YXlmjcV65X0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell N: Self-training on target (pseudo-labeling, verbose & robust) ===\n",
        "import json, numpy as np, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import GradScaler, autocast\n",
        "\n",
        "# ---- config/paths ----\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "SAVE_ROOT = Path(Cfg.SAVE_ROOT)\n",
        "EXP_SELF  = f\"T1.5_SelfTrain_PACS_{Cfg.SOURCE_DOMAIN}2{Cfg.TARGET_DOMAIN}_ResNet50\"\n",
        "OUT_SELF  = make_out_dirs(EXP_SELF)  # creates .../T1.5_SelfTrain_.../{figs,ckpts}\n",
        "\n",
        "# ---- teacher (ERM) ----\n",
        "def _build_teacher_from_ckpt():\n",
        "    # robust: try build_erm() if defined, else rebuild from checkpoint directly\n",
        "    try:\n",
        "        m = build_erm().to(device).eval()\n",
        "        print(\"[SelfTrain] Teacher: build_erm() loaded.\")\n",
        "        return m\n",
        "    except Exception as e:\n",
        "        print(\"[SelfTrain] build_erm() unavailable, loading ERM checkpoint directly:\", e)\n",
        "        m = ResNet50Classifier(num_classes=Cfg.NUM_CLASSES, pretrained=False).to(device)\n",
        "        ckpt = SAVE_ROOT / \"T1.1_SourceOnly_PACS_photo2sketch_ResNet50\" / \"ckpts\" / \"best_by_target.pt\"\n",
        "        m.load_state_dict(torch.load(ckpt, map_location=device))\n",
        "        m.eval()\n",
        "        return m\n",
        "\n",
        "teacher = _build_teacher_from_ckpt()\n",
        "\n",
        "# ---- collect pseudo-labels (with optional tiny TTA) ----\n",
        "def collect_pseudos(dataset, tta=False):\n",
        "    confs, yhat, idxs = [], [], []\n",
        "    teacher.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dataset)):\n",
        "            x, _ = dataset[i]\n",
        "            x = x.unsqueeze(0).to(device, non_blocking=True)\n",
        "            if not tta:\n",
        "                prob = teacher(x).softmax(1).squeeze(0)\n",
        "            else:\n",
        "                probs = []\n",
        "                for flip in [False, True]:\n",
        "                    xb = torch.flip(x, dims=[3]) if flip else x\n",
        "                    probs.append(teacher(xb).softmax(1))\n",
        "                prob = torch.stack(probs, 0).mean(0).squeeze(0)\n",
        "            c, y = prob.max(0)\n",
        "            confs.append(float(c)); yhat.append(int(y)); idxs.append(i)\n",
        "    return np.array(idxs), np.array(yhat, dtype=np.int64), np.array(confs, dtype=np.float32)\n",
        "\n",
        "tgt_ds = loaders[\"tgt_test\"].dataset\n",
        "idx_all, yhat_all, conf_all = collect_pseudos(tgt_ds, tta=False)\n",
        "\n",
        "BASE_THRESH = 0.80\n",
        "MIN_KEEP    = 500    # target minimum pseudo-labeled samples\n",
        "TOP_P       = 0.08   # fallback top-p% overall\n",
        "TOP_K_PER_C = 60     # fallback top-K per class\n",
        "\n",
        "keep = conf_all >= BASE_THRESH\n",
        "kept_idx = idx_all[keep]; kept_y = yhat_all[keep]; kept_conf = conf_all[keep]\n",
        "\n",
        "if kept_idx.size < MIN_KEEP:\n",
        "    for th in [0.75, 0.70, 0.65, 0.60, 0.55]:\n",
        "        keep = conf_all >= th\n",
        "        if keep.sum() >= MIN_KEEP:\n",
        "            kept_idx, kept_y, kept_conf = idx_all[keep], yhat_all[keep], conf_all[keep]\n",
        "            print(f\"[SelfTrain] Adapted threshold -> {th:.2f} (kept {keep.sum()} samples)\")\n",
        "            break\n",
        "\n",
        "if kept_idx.size < MIN_KEEP:\n",
        "    k = max(int(TOP_P * len(conf_all)), MIN_KEEP // 2)\n",
        "    top = np.argsort(-conf_all)[:k]\n",
        "    kept_idx, kept_y, kept_conf = idx_all[top], yhat_all[top], conf_all[top]\n",
        "    print(f\"[SelfTrain] Fallback top-{TOP_P*100:.1f}% overall -> kept {k} samples\")\n",
        "\n",
        "if kept_idx.size < MIN_KEEP:\n",
        "    by_cls = defaultdict(list)\n",
        "    for i, y, c in zip(idx_all, yhat_all, conf_all):\n",
        "        by_cls[int(y)].append((c, i))\n",
        "    sel_idx, sel_y = [], []\n",
        "    for cls in range(Cfg.NUM_CLASSES):\n",
        "        pairs = sorted(by_cls.get(cls, []), key=lambda t: -t[0])[:TOP_K_PER_C]\n",
        "        sel_idx.extend([i for _, i in pairs])\n",
        "        sel_y.extend([cls]*len(pairs))\n",
        "    if len(sel_idx) > kept_idx.size:\n",
        "        kept_idx = np.array(sel_idx, dtype=np.int64)\n",
        "        kept_y   = np.array(sel_y, dtype=np.int64)\n",
        "        print(f\"[SelfTrain] Fallback per-class top-{TOP_K_PER_C} -> kept {len(sel_idx)} samples\")\n",
        "\n",
        "print(f\"[SelfTrain] Pseudo-labeled target: {kept_idx.size} / {len(tgt_ds)} kept (â‰¥{BASE_THRESH:.2f} or adaptive)\")\n",
        "\n",
        "# class histogram for transparency\n",
        "if kept_idx.size > 0:\n",
        "    cls_hist = Counter(kept_y.tolist())\n",
        "    print(\"[SelfTrain] Pseudo-label class histogram (kept):\")\n",
        "    for c in range(Cfg.NUM_CLASSES):\n",
        "        print(f\"  {IDX2CLASS[c]:>14}: {cls_hist.get(c, 0)}\")\n",
        "else:\n",
        "    print(\"[SelfTrain] Pseudo-label set is empty after adaptation.\")\n",
        "\n",
        "# === guard: if still zero, SKIP self-training cleanly ===\n",
        "if kept_idx.size == 0:\n",
        "    summary = {\n",
        "        \"exp_name\": EXP_SELF,\n",
        "        \"domains\": {\"source\": Cfg.SOURCE_DOMAIN, \"target\": Cfg.TARGET_DOMAIN},\n",
        "        \"method\": \"Self-Training (skipped)\",\n",
        "        \"metrics\": {\"target_acc\": float('nan'), \"kept_fraction\": 0.0, \"conf_thresh\": BASE_THRESH},\n",
        "        \"artifacts\": {}\n",
        "    }\n",
        "    with open(OUT_SELF / \"summary_selftrain.json\",\"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f\"[SelfTrain] Skipped. Summary saved -> {OUT_SELF/'summary_selftrain.json'}\")\n",
        "else:\n",
        "    # ---- build pseudo-labeled loader ----\n",
        "    class PseudoTarget(Dataset):\n",
        "        def __init__(self, base_ds, keep_idx, labels):\n",
        "            self.base, self.keep, self.labels = base_ds, list(map(int, keep_idx)), list(map(int, labels))\n",
        "        def __len__(self): return len(self.keep)\n",
        "        def __getitem__(self, k):\n",
        "            i = self.keep[k]; x, _ = self.base[i]\n",
        "            return x, int(self.labels[k])\n",
        "\n",
        "    pl_loader = DataLoader(\n",
        "        PseudoTarget(tgt_ds, kept_idx, kept_y),\n",
        "        batch_size=min(64, Cfg.BATCH_SIZE),\n",
        "        shuffle=True, num_workers=0, pin_memory=False\n",
        "    )\n",
        "\n",
        "    # ---- student: start from ERM; freeze all but final FC ----\n",
        "    student = ResNet50Classifier(num_classes=Cfg.NUM_CLASSES, pretrained=False).to(device)\n",
        "    erm_ckpt = SAVE_ROOT / \"T1.1_SourceOnly_PACS_photo2sketch_ResNet50\" / \"ckpts\" / \"best_by_target.pt\"\n",
        "    student.load_state_dict(torch.load(erm_ckpt, map_location=device))\n",
        "    for n,p in student.named_parameters():\n",
        "        p.requires_grad = (\"backbone.fc\" in n)\n",
        "\n",
        "    LR_ST, WD_ST, EPOCHS_ST = 1e-3, 1e-4, 5\n",
        "    optim  = torch.optim.SGD(filter(lambda p: p.requires_grad, student.parameters()),\n",
        "                             lr=LR_ST, momentum=0.9, weight_decay=WD_ST, nesterov=True)\n",
        "    scaler = GradScaler('cuda', enabled=torch.cuda.is_available())\n",
        "    crit   = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"[SelfTrain] Starting FC-only fine-tune for {EPOCHS_ST} epochs on {len(pl_loader.dataset)} samples\")\n",
        "    print(f\"[SelfTrain] Optim: SGD(lr={LR_ST}, wd={WD_ST}); Batch={pl_loader.batch_size}\")\n",
        "\n",
        "    history_st = {\"epoch\": [], \"tgt_acc\": []}\n",
        "    best_tgt = -1.0\n",
        "\n",
        "    for ep in range(1, EPOCHS_ST+1):\n",
        "        student.train()\n",
        "        running = 0.0\n",
        "        seen = 0\n",
        "        for step, (xb, yb) in enumerate(pl_loader, start=1):\n",
        "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "            with autocast('cuda', enabled=torch.cuda.is_available()):\n",
        "                logits = student(xb)\n",
        "                loss = crit(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optim); scaler.update()\n",
        "\n",
        "            running += loss.item() * yb.size(0)\n",
        "            seen += yb.size(0)\n",
        "            if step % 10 == 0 or step == len(pl_loader):\n",
        "                avg_loss = running / max(seen, 1)\n",
        "                print(f\"[ST {ep:02d}/{EPOCHS_ST}] step {step:04d}/{len(pl_loader):04d} | avg_loss={avg_loss:.4f}\")\n",
        "\n",
        "        # end-of-epoch eval\n",
        "        tgt_acc, _, _ = evaluate(student, loaders[\"tgt_test\"], device)\n",
        "        history_st[\"epoch\"].append(ep); history_st[\"tgt_acc\"].append(float(tgt_acc))\n",
        "        print(f\"[ST {ep:02d}/{EPOCHS_ST}] Target Acc = {tgt_acc:.2f}%\")\n",
        "\n",
        "        # save best\n",
        "        (OUT_SELF / \"ckpts\").mkdir(parents=True, exist_ok=True)\n",
        "        if tgt_acc > best_tgt:\n",
        "            best_tgt = tgt_acc\n",
        "            torch.save(student.state_dict(), OUT_SELF / \"ckpts\" / \"best_selftrain.pt\")\n",
        "            print(f\"[ST] âœ” Saved new best checkpoint @ {best_tgt:.2f}%\")\n",
        "\n",
        "    # ---- curves + artifacts + summary ----\n",
        "    save_json(history_st, OUT_SELF / \"history_selftrain.json\")\n",
        "    (OUT_SELF / \"figs\").mkdir(parents=True, exist_ok=True)\n",
        "    plt.figure(); plt.plot(history_st[\"epoch\"], history_st[\"tgt_acc\"], marker=\"o\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Target Acc (%)\")\n",
        "    plt.title(\"Self-Training on Target (FC-only)\")\n",
        "    plt.savefig(OUT_SELF / \"figs\" / \"selftrain_tgt_curve.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "\n",
        "    # reproducibility for the pseudo set\n",
        "    np.save(OUT_SELF / \"pseudo_idx.npy\", kept_idx)\n",
        "    np.save(OUT_SELF / \"pseudo_y.npy\",   kept_y)\n",
        "\n",
        "    summary = {\n",
        "        \"exp_name\": EXP_SELF,\n",
        "        \"domains\": {\"source\": Cfg.SOURCE_DOMAIN, \"target\": Cfg.TARGET_DOMAIN},\n",
        "        \"method\": \"Self-Training (pseudo-labels; FC-only)\",\n",
        "        \"metrics\": {\n",
        "            \"target_acc\": float(best_tgt),\n",
        "            \"kept_fraction\": float(len(kept_idx) / max(1, len(tgt_ds))),\n",
        "            \"conf_thresh\": BASE_THRESH\n",
        "        },\n",
        "        \"artifacts\": {\n",
        "            \"ckpt_selftrain\": str(OUT_SELF / \"ckpts\" / \"best_selftrain.pt\"),\n",
        "            \"history\":        str(OUT_SELF / \"history_selftrain.json\"),\n",
        "            \"curve\":          str(OUT_SELF / \"figs\" / \"selftrain_tgt_curve.png\"),\n",
        "            \"pseudo_idx\":     str(OUT_SELF / \"pseudo_idx.npy\"),\n",
        "            \"pseudo_y\":       str(OUT_SELF / \"pseudo_y.npy\")\n",
        "        }\n",
        "    }\n",
        "    with open(OUT_SELF / \"summary_selftrain.json\", \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"[SelfTrain] Finished. Best target acc: {best_tgt:.2f}%. Artifacts -> {OUT_SELF}\")\n"
      ],
      "metadata": {
        "id": "aevPRnbu5ab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd33bce-67fa-4bd5-fc81-8a39d8419b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SelfTrain] Teacher: build_erm() loaded.\n",
            "[SelfTrain] Fallback top-8.0% overall -> kept 314 samples\n",
            "[SelfTrain] Pseudo-labeled target: 314 / 3929 kept (â‰¥0.80 or adaptive)\n",
            "[SelfTrain] Pseudo-label class histogram (kept):\n",
            "             dog: 0\n",
            "        elephant: 0\n",
            "         giraffe: 3\n",
            "          guitar: 279\n",
            "           horse: 14\n",
            "           house: 0\n",
            "          person: 18\n",
            "[SelfTrain] Starting FC-only fine-tune for 5 epochs on 314 samples\n",
            "[SelfTrain] Optim: SGD(lr=0.001, wd=0.0001); Batch=32\n",
            "[ST 01/5] step 0010/0010 | avg_loss=1.5788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ST 01/5] Target Acc = 22.09%\n",
            "[ST] âœ” Saved new best checkpoint @ 22.09%\n",
            "[ST 02/5] step 0010/0010 | avg_loss=0.8491\n",
            "[ST 02/5] Target Acc = 15.47%\n",
            "[ST 03/5] step 0010/0010 | avg_loss=0.5659\n",
            "[ST 03/5] Target Acc = 15.47%\n",
            "[ST 04/5] step 0010/0010 | avg_loss=0.4966\n",
            "[ST 04/5] Target Acc = 15.47%\n",
            "[ST 05/5] step 0010/0010 | avg_loss=0.4855\n",
            "[ST 05/5] Target Acc = 15.47%\n",
            "[SelfTrain] Finished. Best target acc: 22.09%. Artifacts -> /content/drive/MyDrive/DG_PACS/Task1/T1.5_SelfTrain_PACS_photo2sketch_ResNet50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell A: Self-Training (Primary, ERM teacher; simple & in-scope) ===\n",
        "import json, numpy as np, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "SAVE_ROOT = Path(Cfg.SAVE_ROOT)\n",
        "EXP_SELF  = f\"T1.5_SelfTrain_PACS_{Cfg.SOURCE_DOMAIN}2{Cfg.TARGET_DOMAIN}_ResNet50_PRIMARY\"\n",
        "OUT_SELF  = (SAVE_ROOT / EXP_SELF); (OUT_SELF / \"ckpts\").mkdir(parents=True, exist_ok=True); (OUT_SELF / \"figs\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Teacher = ERM (strict per brief) ---\n",
        "teacher = ResNet50Classifier(num_classes=Cfg.NUM_CLASSES, pretrained=False).to(device)\n",
        "erm_ckpt = SAVE_ROOT / \"T1.1_SourceOnly_PACS_photo2sketch_ResNet50\" / \"ckpts\" / \"best_by_target.pt\"\n",
        "teacher.load_state_dict(torch.load(erm_ckpt, map_location=device)); teacher.eval()\n",
        "for p in teacher.parameters(): p.requires_grad = False\n",
        "\n",
        "# --- Pseudo-labels on target (with tiny hflip TTA) ---\n",
        "tgt_ds = loaders[\"tgt_test\"].dataset\n",
        "@torch.no_grad()\n",
        "def collect_pseudos(model, dataset, tta=True):\n",
        "    idxs, yhat, confs = [], [], []\n",
        "    model.eval()\n",
        "    for i in range(len(dataset)):\n",
        "        x, _ = dataset[i]\n",
        "        x = x.unsqueeze(0).to(device, non_blocking=True)\n",
        "        if not tta:\n",
        "            prob = model(x).softmax(1).squeeze(0)\n",
        "        else:\n",
        "            p1 = model(x).softmax(1)\n",
        "            p2 = model(torch.flip(x, dims=[3])).softmax(1)\n",
        "            prob = ((p1 + p2) / 2).squeeze(0)\n",
        "        c, y = prob.max(0)\n",
        "        idxs.append(i); yhat.append(int(y)); confs.append(float(c))\n",
        "    return np.array(idxs), np.array(yhat, np.int64), np.array(confs, np.float32)\n",
        "\n",
        "idx_all, yhat_all, conf_all = collect_pseudos(teacher, tgt_ds, tta=True)\n",
        "\n",
        "# --- Adaptive + class-balanced selection (simple) ---\n",
        "BASE_TAU = 0.70\n",
        "PER_CLASS_K = 120\n",
        "MIN_TOTAL = 800\n",
        "\n",
        "def select_adaptive_balanced(idxs, yhat, conf, num_classes):\n",
        "    tau = BASE_TAU\n",
        "    for _ in range(5):\n",
        "        mask = conf >= tau\n",
        "        by_cls = defaultdict(list)\n",
        "        for i, y, c in zip(idxs[mask], yhat[mask], conf[mask]):\n",
        "            by_cls[int(y)].append((c, i))\n",
        "        kept_idx, kept_y, kept_conf = [], [], []\n",
        "        for cls in range(num_classes):\n",
        "            pairs = sorted(by_cls.get(cls, []), key=lambda t: -t[0])[:PER_CLASS_K]\n",
        "            for c, i in pairs:\n",
        "                kept_idx.append(i); kept_y.append(cls); kept_conf.append(c)\n",
        "        if len(kept_idx) >= MIN_TOTAL:\n",
        "            break\n",
        "        tau = max(0.50, tau - 0.05)\n",
        "    print(f\"[SelfTrain PRIMARY] tauâ‰ˆ{tau:.2f} | kept={len(kept_idx)} | per-class:\", dict(Counter(kept_y)))\n",
        "    return np.array(kept_idx, np.int64), np.array(kept_y, np.int64), np.array(kept_conf, np.float32), float(tau)\n",
        "\n",
        "kept_idx, kept_y, kept_conf, used_tau = select_adaptive_balanced(idx_all, yhat_all, conf_all, Cfg.NUM_CLASSES)\n",
        "\n",
        "class PseudoTarget(Dataset):\n",
        "    def __init__(self, base_ds, keep_idx, labels):\n",
        "        self.base, self.keep, self.labels = base_ds, list(map(int, keep_idx)), list(map(int, labels))\n",
        "    def __len__(self): return len(self.keep)\n",
        "    def __getitem__(self, k):\n",
        "        i = self.keep[k]; x, _ = self.base[i]\n",
        "        return x, int(self.labels[k])\n",
        "\n",
        "pl_loader = DataLoader(PseudoTarget(tgt_ds, kept_idx, kept_y),\n",
        "                       batch_size=min(64, Cfg.BATCH_SIZE), shuffle=True, num_workers=0, pin_memory=False)\n",
        "\n",
        "# --- Student init from ERM; 2 phases: FC-only â†’ unfreeze layer4 (tiny LR) ---\n",
        "def build_student():\n",
        "    m = ResNet50Classifier(num_classes=Cfg.NUM_CLASSES, pretrained=False).to(device)\n",
        "    m.load_state_dict(torch.load(erm_ckpt, map_location=device))\n",
        "    return m\n",
        "\n",
        "def set_trainable(student, phase):\n",
        "    for p in student.parameters(): p.requires_grad = False\n",
        "    if phase >= 1:\n",
        "        for n,p in student.named_parameters():\n",
        "            if \"backbone.fc\" in n: p.requires_grad = True\n",
        "    if phase >= 2:\n",
        "        for n,p in student.named_parameters():\n",
        "            if \"backbone.layer4\" in n or \"backbone.fc\" in n: p.requires_grad = True\n",
        "\n",
        "LR_FC, LR_L4, WD, MOM = 1e-3, 3e-4, 1e-4, 0.9\n",
        "E1, E2 = 3, 2\n",
        "student = build_student()\n",
        "\n",
        "history = {\"epoch\": [], \"tgt_acc\": [], \"phase\": []}\n",
        "best_state = None; best_tgt = -1.0\n",
        "\n",
        "def eval_src_tgt(model):\n",
        "    src_acc, _, _ = evaluate(model, loaders[\"src_test\"], device)\n",
        "    tgt_acc, _, _ = evaluate(model, loaders[\"tgt_test\"], device)\n",
        "    return float(src_acc), float(tgt_acc)\n",
        "\n",
        "# Phase 1: FC-only\n",
        "set_trainable(student, phase=1)\n",
        "opt = torch.optim.SGD([{\"params\":[p for n,p in student.named_parameters() if p.requires_grad and \"backbone.fc\" in n], \"lr\": LR_FC}],\n",
        "                      momentum=MOM, weight_decay=WD, nesterov=True)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "for ep in range(1, E1+1):\n",
        "    student.train()\n",
        "    for xb, yb in pl_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss = crit(student(xb), yb)\n",
        "        loss.backward(); nn.utils.clip_grad_norm_(student.parameters(), 5.0); opt.step()\n",
        "    _, tgt_acc = eval_src_tgt(student)\n",
        "    history[\"epoch\"].append(len(history[\"epoch\"])+1); history[\"tgt_acc\"].append(tgt_acc); history[\"phase\"].append(1)\n",
        "    print(f\"[PRIMARY P1 E{ep}/{E1}] tgt_acc={tgt_acc:.2f}%\")\n",
        "    if tgt_acc > best_tgt: best_tgt = tgt_acc; best_state = student.state_dict().copy()\n",
        "\n",
        "# Phase 2: unfreeze layer4\n",
        "set_trainable(student, phase=2)\n",
        "opt = torch.optim.SGD(\n",
        "    [\n",
        "        {\"params\":[p for n,p in student.named_parameters() if p.requires_grad and \"backbone.fc\" in n], \"lr\": LR_FC},\n",
        "        {\"params\":[p for n,p in student.named_parameters() if p.requires_grad and \"backbone.layer4\" in n], \"lr\": LR_L4},\n",
        "    ],\n",
        "    momentum=MOM, weight_decay=WD, nesterov=True\n",
        ")\n",
        "for ep in range(1, E2+1):\n",
        "    student.train()\n",
        "    for xb, yb in pl_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss = crit(student(xb), yb)\n",
        "        loss.backward(); nn.utils.clip_grad_norm_(student.parameters(), 5.0); opt.step()\n",
        "    _, tgt_acc = eval_src_tgt(student)\n",
        "    history[\"epoch\"].append(len(history[\"epoch\"])+1); history[\"tgt_acc\"].append(tgt_acc); history[\"phase\"].append(2)\n",
        "    print(f\"[PRIMARY P2 E{ep}/{E2}] tgt_acc={tgt_acc:.2f}%\")\n",
        "    if tgt_acc > best_tgt: best_tgt = tgt_acc; best_state = student.state_dict().copy()\n",
        "\n",
        "# Save best checkpoint and compute final metrics\n",
        "if best_state is not None: student.load_state_dict(best_state)\n",
        "src_acc, tgt_acc = eval_src_tgt(student)\n",
        "avg_domain_acc = float((src_acc + tgt_acc) / 2.0)\n",
        "worst_group_acc = float(min(src_acc, tgt_acc))\n",
        "kept_fraction = float(len(kept_idx) / max(1, len(tgt_ds)))\n",
        "\n",
        "torch.save(student.state_dict(), OUT_SELF / \"ckpts\" / \"best_selftrain_primary.pt\")\n",
        "with open(OUT_SELF / \"history_selftrain_primary.json\", \"w\") as f:\n",
        "    json.dump({\"phase\": history[\"phase\"], \"epoch\": history[\"epoch\"], \"tgt_acc\": history[\"tgt_acc\"]}, f, indent=2)\n",
        "\n",
        "plt.figure(); plt.plot(history[\"epoch\"], history[\"tgt_acc\"], marker=\"o\")\n",
        "plt.xlabel(\"Epoch (P1+P2)\"); plt.ylabel(\"Target Acc (%)\"); plt.title(\"Self-Training (Primary, ERM teacher)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig(OUT_SELF / \"figs\" / \"selftrain_primary_curve.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "\n",
        "summary = {\n",
        "    \"exp_name\": EXP_SELF,\n",
        "    \"domains\": {\"source\": Cfg.SOURCE_DOMAIN, \"target\": Cfg.TARGET_DOMAIN},\n",
        "    \"method\": \"Self-Training (Primary, ERM teacher)\",\n",
        "    \"selection\": {\"tau_used\": used_tau, \"per_class_k\": PER_CLASS_K, \"min_total\": MIN_TOTAL,\n",
        "                  \"kept_fraction\": kept_fraction, \"kept_count\": int(len(kept_idx))},\n",
        "    \"metrics\": {\n",
        "        \"source_acc\": src_acc,\n",
        "        \"target_acc\": tgt_acc,\n",
        "        \"avg_domain_acc\": avg_domain_acc,\n",
        "        \"worst_group_acc\": worst_group_acc\n",
        "    },\n",
        "    \"artifacts\": {\n",
        "        \"ckpt_best\": str(OUT_SELF / \"ckpts\" / \"best_selftrain_primary.pt\"),\n",
        "        \"history\": str(OUT_SELF / \"history_selftrain_primary.json\"),\n",
        "        \"curve\": str(OUT_SELF / \"figs\" / \"selftrain_primary_curve.png\")\n",
        "    },\n",
        "    \"notes\": \"ERM teacher; simple adaptive + per-class selection; FC warm-up â†’ tiny layer4 unfreeze.\"\n",
        "}\n",
        "with open(OUT_SELF / \"summary_selftrain_primary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(json.dumps(summary[\"metrics\"], indent=2))\n",
        "print(f\"[PRIMARY] Saved artifacts -> {OUT_SELF}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwImI5jV93Sh",
        "outputId": "17bde779-b6ce-4515-ad6e-79c0cd9ccf4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SelfTrain PRIMARY] tauâ‰ˆ0.50 | kept=4 | per-class: {3: 4}\n",
            "[PRIMARY P1 E1/3] tgt_acc=21.35%\n",
            "[PRIMARY P1 E2/3] tgt_acc=23.92%\n",
            "[PRIMARY P1 E3/3] tgt_acc=25.30%\n",
            "[PRIMARY P2 E1/2] tgt_acc=27.36%\n",
            "[PRIMARY P2 E2/2] tgt_acc=28.48%\n",
            "{\n",
            "  \"source_acc\": 95.02994011976048,\n",
            "  \"target_acc\": 28.480529396793077,\n",
            "  \"avg_domain_acc\": 61.75523475827678,\n",
            "  \"worst_group_acc\": 28.480529396793077\n",
            "}\n",
            "[PRIMARY] Saved artifacts -> /content/drive/MyDrive/DG_PACS/Task1/T1.5_SelfTrain_PACS_photo2sketch_ResNet50_PRIMARY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell B: Self-Training (Ablation, DANN teacher; robust selection + early stopping) ===\n",
        "import json, numpy as np, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "SAVE_ROOT = Path(Cfg.SAVE_ROOT)\n",
        "EXP_ABL  = f\"T1.5_SelfTrain_PACS_{Cfg.SOURCE_DOMAIN}2{Cfg.TARGET_DOMAIN}_ResNet50_ABL_DANNteacher\"\n",
        "OUT_ABL  = (SAVE_ROOT / EXP_ABL); (OUT_ABL / \"ckpts\").mkdir(parents=True, exist_ok=True); (OUT_ABL / \"figs\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Teacher = DANN backbone (features + fc) transplanted into ResNet50Classifier ---\n",
        "teacher = ResNet50Classifier(num_classes=Cfg.NUM_CLASSES, pretrained=False).to(device)\n",
        "dann_bb = SAVE_ROOT / \"T1.2_DANN_PACS_photo2sketch_ResNet50_memlite\" / \"ckpts\" / \"best_dann_backbone.pt\"\n",
        "if not dann_bb.exists():\n",
        "    raise FileNotFoundError(\"DANN backbone not found; run Task 1.2 DANN first.\")\n",
        "\n",
        "# Load DANN backbone module to read weights (includes classifier layer)\n",
        "feat = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=False).to(device)\n",
        "feat.load_state_dict(torch.load(dann_bb, map_location=device))\n",
        "# Transfer conv/bn/layers and fc weights into the classifier model\n",
        "base = teacher.backbone\n",
        "# features: [0]conv1, [1]bn1, [2]relu, [3]maxpool, [4]layer1, [5]layer2, [6]layer3, [7]layer4, [8]avgpool\n",
        "base.conv1.load_state_dict(feat.features[0].state_dict())\n",
        "base.bn1.load_state_dict(feat.features[1].state_dict())\n",
        "base.layer1.load_state_dict(feat.features[4].state_dict())\n",
        "base.layer2.load_state_dict(feat.features[5].state_dict())\n",
        "base.layer3.load_state_dict(feat.features[6].state_dict())\n",
        "base.layer4.load_state_dict(feat.features[7].state_dict())\n",
        "# fc\n",
        "with torch.no_grad():\n",
        "    base.fc.weight.copy_(feat.classifier.weight)\n",
        "    base.fc.bias.copy_(feat.classifier.bias)\n",
        "\n",
        "teacher.eval();  [p.requires_grad_(False) for p in teacher.parameters()]\n",
        "print(\"[SelfTrain ABL] DANN teacher loaded (features + fc).\")\n",
        "\n",
        "# --- Pseudo-labels on target (with hflip TTA) ---\n",
        "tgt_ds = loaders[\"tgt_test\"].dataset\n",
        "@torch.no_grad()\n",
        "def collect_pseudos(model, dataset, tta=True):\n",
        "    idxs, yhat, confs = [], [], []\n",
        "    model.eval()\n",
        "    for i in range(len(dataset)):\n",
        "        x, _ = dataset[i]\n",
        "        x = x.unsqueeze(0).to(device, non_blocking=True)\n",
        "        if not tta:\n",
        "            prob = model(x).softmax(1).squeeze(0)\n",
        "        else:\n",
        "            p1 = model(x).softmax(1)\n",
        "            p2 = model(torch.flip(x, dims=[3])).softmax(1)\n",
        "            prob = ((p1 + p2) / 2).squeeze(0)\n",
        "        c, y = prob.max(0)\n",
        "        idxs.append(i); yhat.append(int(y)); confs.append(float(c))\n",
        "    return np.array(idxs), np.array(yhat, np.int64), np.array(confs, np.float32)\n",
        "\n",
        "idx_all, yhat_all, conf_all = collect_pseudos(teacher, tgt_ds, tta=True)\n",
        "\n",
        "# --- Adaptive + class-balanced selection, with robust fallbacks ---\n",
        "BASE_TAU = 0.70\n",
        "PER_CLASS_K = 120\n",
        "MIN_TOTAL = 800\n",
        "TOP_P = 0.20            # 20% overall fallback\n",
        "MIN_PER_CLASS = 80      # ensure some coverage per class in fallback\n",
        "\n",
        "def select_adaptive_balanced_robust(idxs, yhat, conf, num_classes):\n",
        "    # 1) Try thresholding and per-class top-K\n",
        "    tau = BASE_TAU\n",
        "    kept_idx, kept_y, kept_conf = [], [], []\n",
        "    for _ in range(5):\n",
        "        mask = conf >= tau\n",
        "        by_cls = defaultdict(list)\n",
        "        for i, y, c in zip(idxs[mask], yhat[mask], conf[mask]):\n",
        "            by_cls[int(y)].append((c, i))\n",
        "        cur_idx, cur_y, cur_conf = [], [], []\n",
        "        for cls in range(num_classes):\n",
        "            pairs = sorted(by_cls.get(cls, []), key=lambda t: -t[0])[:PER_CLASS_K]\n",
        "            for c, i in pairs: cur_idx.append(i); cur_y.append(cls); cur_conf.append(c)\n",
        "        if len(cur_idx) >= MIN_TOTAL:\n",
        "            kept_idx, kept_y, kept_conf = cur_idx, cur_y, cur_conf\n",
        "            print(f\"[SelfTrain ABL] tauâ‰ˆ{tau:.2f} | kept={len(kept_idx)} | per-class:\", dict(Counter(cur_y)))\n",
        "            return np.array(kept_idx, np.int64), np.array(kept_y, np.int64), np.array(cur_conf, np.float32), float(tau)\n",
        "        tau = max(0.50, tau - 0.05)\n",
        "\n",
        "    # 2) Fallback: top-p overall by confidence\n",
        "    k = max(int(TOP_P * len(conf)), MIN_TOTAL)\n",
        "    top = np.argsort(-conf)[:k]\n",
        "    kept_idx, kept_y, kept_conf = idxs[top], yhat[top], conf[top]\n",
        "    print(f\"[SelfTrain ABL] Fallback top-{int(TOP_P*100)}% overall -> kept={len(kept_idx)}\")\n",
        "\n",
        "    # 3) Ensure per-class coverage by backfilling classes below MIN_PER_CLASS\n",
        "    counts = Counter(kept_y.tolist())\n",
        "    if any(counts[c] < MIN_PER_CLASS for c in range(num_classes)):\n",
        "        remaining = np.setdiff1d(np.arange(len(conf)), top, assume_unique=False)\n",
        "        pool_by_cls = defaultdict(list)\n",
        "        for j in remaining:\n",
        "            pool_by_cls[int(yhat[j])].append((conf[j], idxs[j]))\n",
        "        kept_idx = kept_idx.tolist(); kept_y = kept_y.tolist(); kept_conf = kept_conf.tolist()\n",
        "        for cls in range(num_classes):\n",
        "            need = max(0, MIN_PER_CLASS - counts.get(cls, 0))\n",
        "            if need > 0 and len(pool_by_cls[cls]) > 0:\n",
        "                extra = sorted(pool_by_cls[cls], key=lambda t: -t[0])[:need]\n",
        "                for c,i in extra:\n",
        "                    kept_idx.append(i); kept_y.append(cls); kept_conf.append(c)\n",
        "        kept_idx = np.array(kept_idx, np.int64); kept_y = np.array(kept_y, np.int64); kept_conf = np.array(kept_conf, np.float32)\n",
        "        print(f\"[SelfTrain ABL] Backfilled per-class to ensure coverage | per-class:\", dict(Counter(kept_y.tolist())))\n",
        "\n",
        "    # 4) If still zero (extremely unlikely), return empty and caller will skip\n",
        "    if len(kept_idx) == 0:\n",
        "        print(\"[SelfTrain ABL] Pseudo set is EMPTY after all fallbacks.\")\n",
        "        return kept_idx, kept_y, kept_conf, float(tau)\n",
        "\n",
        "    return kept_idx, kept_y, kept_conf, float(tau)\n",
        "\n",
        "kept_idx, kept_y, kept_conf, used_tau = select_adaptive_balanced_robust(idx_all, yhat_all, conf_all, Cfg.NUM_CLASSES)\n",
        "\n",
        "# --- Guard: handle empty set cleanly (skip training, save summary) ---\n",
        "if len(kept_idx) == 0:\n",
        "    summary = {\n",
        "        \"exp_name\": EXP_ABL,\n",
        "        \"domains\": {\"source\": Cfg.SOURCE_DOMAIN, \"target\": Cfg.TARGET_DOMAIN},\n",
        "        \"method\": \"Self-Training (Ablation, DANN teacher)\",\n",
        "        \"selection\": {\"tau_used\": used_tau, \"per_class_k\": PER_CLASS_K, \"min_total\": MIN_TOTAL,\n",
        "                      \"kept_fraction\": 0.0, \"kept_count\": 0},\n",
        "        \"metrics\": {\n",
        "            \"source_acc\": float('nan'),\n",
        "            \"target_acc\": float('nan'),\n",
        "            \"avg_domain_acc\": float('nan'),\n",
        "            \"worst_group_acc\": float('nan')\n",
        "        },\n",
        "        \"artifacts\": {}\n",
        "    }\n",
        "    with open(OUT_ABL / \"summary_selftrain_ablation.json\", \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(\"[ABLATION] Skipped training: no pseudo-labels could be selected. Summary saved.\")\n",
        "else:\n",
        "    class PseudoTarget(Dataset):\n",
        "        def __init__(self, base_ds, keep_idx, labels):\n",
        "            self.base, self.keep, self.labels = base_ds, list(map(int, keep_idx)), list(map(int, labels))\n",
        "        def __len__(self): return len(self.keep)\n",
        "        def __getitem__(self, k):\n",
        "            i = self.keep[k]; x, _ = self.base[i]\n",
        "            return x, int(self.labels[k])\n",
        "\n",
        "    pl_loader = DataLoader(PseudoTarget(tgt_ds, kept_idx, kept_y),\n",
        "                           batch_size=min(64, Cfg.BATCH_SIZE), shuffle=True, num_workers=0, pin_memory=False)\n",
        "\n",
        "    # --- Student = ERM init; 2-phase schedule with early stopping (same as primary style) ---\n",
        "    def build_student():\n",
        "        m = ResNet50Classifier(num_classes=Cfg.NUM_CLASSES, pretrained=False).to(device)\n",
        "        m.load_state_dict(torch.load(SAVE_ROOT / \"T1.1_SourceOnly_PACS_photo2sketch_ResNet50\" / \"ckpts\" / \"best_by_target.pt\",\n",
        "                                     map_location=device))\n",
        "        return m\n",
        "\n",
        "    def set_trainable(student, phase):\n",
        "        for p in student.parameters(): p.requires_grad = False\n",
        "        if phase >= 1:\n",
        "            for n,p in student.named_parameters():\n",
        "                if \"backbone.fc\" in n: p.requires_grad = True\n",
        "        if phase >= 2:\n",
        "            for n,p in student.named_parameters():\n",
        "                if \"backbone.layer4\" in n or \"backbone.fc\" in n: p.requires_grad = True\n",
        "\n",
        "    LR_FC, LR_L4, WD, MOM = 1e-3, 3e-4, 1e-4, 0.9\n",
        "    student = build_student()\n",
        "\n",
        "    history = {\"epoch\": [], \"tgt_acc\": [], \"phase\": []}\n",
        "    best_state = None; best_tgt = -1.0\n",
        "\n",
        "    def eval_src_tgt(model):\n",
        "        src_acc, _, _ = evaluate(model, loaders[\"src_test\"], device)\n",
        "        tgt_acc, _, _ = evaluate(model, loaders[\"tgt_test\"], device)\n",
        "        return float(src_acc), float(tgt_acc)\n",
        "\n",
        "    # ---- Phase 1: FC-only (up to 6 epochs, patience=2) ----\n",
        "    set_trainable(student, phase=1)\n",
        "    opt = torch.optim.SGD(\n",
        "        [{\"params\":[p for n,p in student.named_parameters() if p.requires_grad and \"backbone.fc\" in n], \"lr\": LR_FC}],\n",
        "        momentum=MOM, weight_decay=WD, nesterov=True\n",
        "    )\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    E1, PATIENCE1 = 6, 2\n",
        "    best_tgt_phase = -1.0; no_improve = 0\n",
        "\n",
        "    for ep in range(1, E1+1):\n",
        "        student.train()\n",
        "        for xb, yb in pl_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss = crit(student(xb), yb)\n",
        "            loss.backward(); nn.utils.clip_grad_norm_(student.parameters(), 5.0); opt.step()\n",
        "        _, tgt_acc = eval_src_tgt(student)\n",
        "        history[\"epoch\"].append(len(history[\"epoch\"])+1); history[\"tgt_acc\"].append(tgt_acc); history[\"phase\"].append(1)\n",
        "        print(f\"[ABL P1 E{ep}/{E1}] tgt_acc={tgt_acc:.2f}%\")\n",
        "        if tgt_acc > best_tgt_phase:\n",
        "            best_tgt_phase = tgt_acc; no_improve = 0\n",
        "            if tgt_acc > best_tgt: best_tgt = tgt_acc; best_state = student.state_dict().copy()\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= PATIENCE1:\n",
        "                print(\"[ABL] Early stop Phase 1.\"); break\n",
        "\n",
        "    # ---- Phase 2: unfreeze layer4 (up to 10 epochs, patience=3) ----\n",
        "    set_trainable(student, phase=2)\n",
        "    opt = torch.optim.SGD(\n",
        "        [\n",
        "            {\"params\":[p for n,p in student.named_parameters() if p.requires_grad and \"backbone.fc\" in n], \"lr\": LR_FC},\n",
        "            {\"params\":[p for n,p in student.named_parameters() if p.requires_grad and \"backbone.layer4\" in n], \"lr\": LR_L4},\n",
        "        ],\n",
        "        momentum=MOM, weight_decay=WD, nesterov=True\n",
        "    )\n",
        "    E2, PATIENCE2 = 10, 3\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=E2)\n",
        "    best_tgt_phase = -1.0; no_improve = 0\n",
        "\n",
        "    for ep in range(1, E2+1):\n",
        "        student.train()\n",
        "        for xb, yb in pl_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss = crit(student(xb), yb)\n",
        "            loss.backward(); nn.utils.clip_grad_norm_(student.parameters(), 5.0); opt.step()\n",
        "        sched.step()\n",
        "        _, tgt_acc = eval_src_tgt(student)\n",
        "        history[\"epoch\"].append(len(history[\"epoch\"])+1); history[\"tgt_acc\"].append(tgt_acc); history[\"phase\"].append(2)\n",
        "        print(f\"[ABL P2 E{ep}/{E2}] tgt_acc={tgt_acc:.2f}%\")\n",
        "        if tgt_acc > best_tgt_phase:\n",
        "            best_tgt_phase = tgt_acc; no_improve = 0\n",
        "            if tgt_acc > best_tgt: best_tgt = tgt_acc; best_state = student.state_dict().copy()\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= PATIENCE2:\n",
        "                print(\"[ABL] Early stop Phase 2.\"); break\n",
        "\n",
        "    # ---- Save best + metrics (consistent with other methods) ----\n",
        "    if best_state is not None: student.load_state_dict(best_state)\n",
        "    src_acc, tgt_acc = eval_src_tgt(student)\n",
        "    avg_domain_acc = float((src_acc + tgt_acc) / 2.0)\n",
        "    worst_group_acc = float(min(src_acc, tgt_acc))\n",
        "    kept_fraction = float(len(kept_idx) / max(1, len(tgt_ds)))\n",
        "\n",
        "    torch.save(student.state_dict(), OUT_ABL / \"ckpts\" / \"best_selftrain_ablation_dannT.pt\")\n",
        "    with open(OUT_ABL / \"history_selftrain_ablation.json\", \"w\") as f:\n",
        "        json.dump({\"phase\": history[\"phase\"], \"epoch\": history[\"epoch\"], \"tgt_acc\": history[\"tgt_acc\"]}, f, indent=2)\n",
        "\n",
        "    plt.figure(); plt.plot(history[\"epoch\"], history[\"tgt_acc\"], marker=\"o\")\n",
        "    plt.xlabel(\"Epoch (P1+P2)\"); plt.ylabel(\"Target Acc (%)\"); plt.title(\"Self-Training (Ablation, DANN teacher)\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(OUT_ABL / \"figs\" / \"selftrain_ablation_curve.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "\n",
        "    summary = {\n",
        "        \"exp_name\": EXP_ABL,\n",
        "        \"domains\": {\"source\": Cfg.SOURCE_DOMAIN, \"target\": Cfg.TARGET_DOMAIN},\n",
        "        \"method\": \"Self-Training (Ablation, DANN teacher)\",\n",
        "        \"selection\": {\"tau_used\": used_tau, \"per_class_k\": PER_CLASS_K, \"min_total\": MIN_TOTAL,\n",
        "                      \"kept_fraction\": kept_fraction, \"kept_count\": int(len(kept_idx))},\n",
        "        \"metrics\": {\n",
        "            \"source_acc\": src_acc,\n",
        "            \"target_acc\": tgt_acc,\n",
        "            \"avg_domain_acc\": avg_domain_acc,\n",
        "            \"worst_group_acc\": worst_group_acc\n",
        "        },\n",
        "        \"artifacts\": {\n",
        "            \"ckpt_best\": str(OUT_ABL / \"ckpts\" / \"best_selftrain_ablation_dannT.pt\"),\n",
        "            \"history\": str(OUT_ABL / \"history_selftrain_ablation.json\"),\n",
        "            \"curve\": str(OUT_ABL / \"figs\" / \"selftrain_ablation_curve.png\")\n",
        "        },\n",
        "        \"notes\": \"Ablation only: DANN teacher (features+fc); robust selection; FC warm-up â†’ tiny layer4 unfreeze with early stopping.\"\n",
        "    }\n",
        "    with open(OUT_ABL / \"summary_selftrain_ablation.json\", \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(json.dumps(summary[\"metrics\"], indent=2))\n",
        "    print(f\"[ABLATION] Saved artifacts -> {OUT_ABL}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXd-y6thAKrJ",
        "outputId": "1d9c898f-f524-47e0-faac-3428746c30f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SelfTrain ABL] DANN teacher loaded (features + fc).\n",
            "[SelfTrain ABL] Fallback top-20% overall -> kept=800\n",
            "[SelfTrain ABL] Backfilled per-class to ensure coverage | per-class: {3: 348, 4: 155, 0: 101, 1: 188, 2: 80, 5: 76, 6: 10}\n",
            "[ABL P1 E1/6] tgt_acc=29.96%\n",
            "[ABL P1 E2/6] tgt_acc=36.73%\n",
            "[ABL P1 E3/6] tgt_acc=44.57%\n",
            "[ABL P1 E4/6] tgt_acc=47.98%\n",
            "[ABL P1 E5/6] tgt_acc=51.49%\n",
            "[ABL P1 E6/6] tgt_acc=52.23%\n",
            "[ABL P2 E1/10] tgt_acc=56.30%\n",
            "[ABL P2 E2/10] tgt_acc=58.51%\n",
            "[ABL P2 E3/10] tgt_acc=59.66%\n",
            "[ABL P2 E4/10] tgt_acc=60.09%\n",
            "[ABL P2 E5/10] tgt_acc=60.37%\n",
            "[ABL P2 E6/10] tgt_acc=61.42%\n",
            "[ABL P2 E7/10] tgt_acc=61.90%\n",
            "[ABL P2 E8/10] tgt_acc=61.11%\n",
            "[ABL P2 E9/10] tgt_acc=61.90%\n",
            "[ABL P2 E10/10] tgt_acc=61.42%\n",
            "[ABL] Early stop Phase 2.\n",
            "{\n",
            "  \"source_acc\": 11.497005988023952,\n",
            "  \"target_acc\": 61.41511835072537,\n",
            "  \"avg_domain_acc\": 36.45606216937466,\n",
            "  \"worst_group_acc\": 11.497005988023952\n",
            "}\n",
            "[ABLATION] Saved artifacts -> /content/drive/MyDrive/DG_PACS/Task1/T1.5_SelfTrain_PACS_photo2sketch_ResNet50_ABL_DANNteacher\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE feature plots (source vs target) for each method"
      ],
      "metadata": {
        "id": "8zAEIuUS5xun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell O: t-SNE embeddings per method (source vs target) ===\n",
        "import numpy as np, torch, matplotlib.pyplot as plt, json\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def collect_feats_logits(model, loader, max_n=800):\n",
        "    model.eval()\n",
        "    Fs, Ys, Ds = [], [], []  # feats/logits inference â†’ use penultimate logits as proxy if needed\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            z = model(x)                 # logits; we can t-SNE logits (class-separable space)\n",
        "            Fs.append(z.detach().cpu().float())\n",
        "            Ys.append(y.numpy()); D = np.full(len(y), fill_value=0, dtype=np.int64)  # placeholder\n",
        "            n += len(y)\n",
        "            if n >= max_n: break\n",
        "    return torch.cat(Fs).numpy(), np.concatenate(Ys)\n",
        "\n",
        "def tsne_plot(Fs, labels, title, savepath):\n",
        "    ts = TSNE(n_components=2, init='pca', learning_rate='auto', perplexity=30)\n",
        "    emb = ts.fit_transform(Fs)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sc = plt.scatter(emb[:,0], emb[:,1], c=labels, s=8)\n",
        "    plt.title(title); plt.tight_layout()\n",
        "    plt.savefig(savepath, bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "\n",
        "METHOD_BUILDERS = {\n",
        "    \"ERM\": build_erm,\n",
        "    \"DAN\": build_dan,\n",
        "    \"CDAN\": build_cdan,\n",
        "}\n",
        "# DANN uses backbone+classifier head (already available in your H cell); reuse backbone.classifier path\n",
        "def build_dann_clf():\n",
        "    b = build_dann_backbone()\n",
        "    class C(nn.Module):\n",
        "        def __init__(self,b): super().__init__(); self.b=b\n",
        "        def forward(self,x):\n",
        "            f = self.b(x, return_feat=False, class_head=False)\n",
        "            return self.b.classifier(f)\n",
        "    return C(b).to(device).eval()\n",
        "\n",
        "METHOD_BUILDERS[\"DANN\"] = build_dann_clf\n",
        "\n",
        "OUT_TSNE = Path(Cfg.SAVE_ROOT)/\"T1_TSNE\"\n",
        "(OUT_TSNE).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for name, builder in METHOD_BUILDERS.items():\n",
        "    clf = builder()\n",
        "    Fs_src, y_src = collect_feats_logits(clf, loaders[\"src_test\"], max_n=800)\n",
        "    Fs_tgt, y_tgt = collect_feats_logits(clf, loaders[\"tgt_test\"], max_n=800)\n",
        "    tsne_plot(np.vstack([Fs_src, Fs_tgt]),\n",
        "              np.concatenate([y_src, y_tgt]),\n",
        "              f\"t-SNE logits: {name} (src+tg)\",\n",
        "              OUT_TSNE/f\"tsne_{name}_logits_src_tgt.png\")\n",
        "    print(f\"Saved t-SNE for {name} -> {OUT_TSNE/f'tsne_{name}_logits_src_tgt.png'}\")\n"
      ],
      "metadata": {
        "id": "x2O5WBbR5f_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eace460-3199-4df0-d37f-518da0ea888d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved t-SNE for ERM -> /content/drive/MyDrive/DG_PACS/Task1/T1_TSNE/tsne_ERM_logits_src_tgt.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved t-SNE for DAN -> /content/drive/MyDrive/DG_PACS/Task1/T1_TSNE/tsne_DAN_logits_src_tgt.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved t-SNE for CDAN -> /content/drive/MyDrive/DG_PACS/Task1/T1_TSNE/tsne_CDAN_logits_src_tgt.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved t-SNE for DANN -> /content/drive/MyDrive/DG_PACS/Task1/T1_TSNE/tsne_DANN_logits_src_tgt.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell P: Label/Concept shift & rare-class stress tests ===\n",
        "import numpy as np, torch, json\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "def downsample_classes(base_loader, keep_ratio_by_class, max_per_cls=None):\n",
        "    ds = base_loader.dataset\n",
        "    idx_by_cls = {}\n",
        "    for i in range(len(ds)):\n",
        "        _, y = ds[i]; idx_by_cls.setdefault(int(y), []).append(i)\n",
        "    new_idx = []\n",
        "    for c, idxs in idx_by_cls.items():\n",
        "        k = int(len(idxs) * keep_ratio_by_class.get(c, 1.0))\n",
        "        if max_per_cls: k = min(k, max_per_cls)\n",
        "        new_idx.extend(idxs[:max(k,1)])\n",
        "    return DataLoader(Subset(ds, new_idx), batch_size=base_loader.batch_size, shuffle=False,\n",
        "                      num_workers=0, pin_memory=False)\n",
        "\n",
        "def eval_all_methods(loader, note):\n",
        "    results = {}\n",
        "    # ERM, DAN, CDAN, DANN head\n",
        "    models = {\n",
        "        \"ERM\": build_erm(),\n",
        "        \"DAN\": build_dan(),\n",
        "        \"CDAN\": build_cdan(),\n",
        "        \"DANN\": build_dann_clf(),\n",
        "    }\n",
        "    for name, m in models.items():\n",
        "        acc, _, _ = evaluate(m, loader, device)\n",
        "        results[name] = float(acc)\n",
        "    return results\n",
        "\n",
        "# 1) Label shift: e.g., shrink 3 classes on target to 20% prevalence\n",
        "rare_classes = [0, 1, 2]  # pick any three; or choose programmatically by counts\n",
        "keep_ratio = {c: 0.2 for c in rare_classes}\n",
        "tgt_shift_loader = downsample_classes(loaders[\"tgt_test\"], keep_ratio)\n",
        "res_labelshift = eval_all_methods(tgt_shift_loader, \"label_shift\")\n",
        "\n",
        "# 2) Rare-class stress: keep only 10% of the rarest class on target\n",
        "#    (find rarest class first)\n",
        "counts = Counter()\n",
        "for _, y in loaders[\"tgt_test\"]:\n",
        "    for v in y.tolist(): counts[int(v)] += 1\n",
        "rarest = min(counts.keys(), key=lambda c: counts[c])\n",
        "keep_ratio2 = {rarest: 0.1}\n",
        "tgt_rare_loader = downsample_classes(loaders[\"tgt_test\"], keep_ratio2)\n",
        "res_rarestress = eval_all_methods(tgt_rare_loader, \"rare_stress\")\n",
        "\n",
        "# Save JSON roll-up\n",
        "OUT_SHIFT = Path(Cfg.SAVE_ROOT)/\"T1_ShiftStress\"\n",
        "OUT_SHIFT.mkdir(parents=True, exist_ok=True)\n",
        "with open(OUT_SHIFT/\"label_shift_results.json\",\"w\") as f: json.dump(res_labelshift, f, indent=2)\n",
        "with open(OUT_SHIFT/\"rare_stress_results.json\",\"w\") as f: json.dump(res_rarestress, f, indent=2)\n",
        "\n",
        "print(\"Label-shift target acc (%):\", res_labelshift)\n",
        "print(\"Rare-class stress target acc (%):\", res_rarestress)\n",
        "print(f\"Saved shift stress results under {OUT_SHIFT}\")\n"
      ],
      "metadata": {
        "id": "RlbqwF0G6SIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41990634-750e-4325-c88f-6b95d3cf9218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label-shift target acc (%): {'ERM': 41.351606805293, 'DAN': 12.145557655954631, 'CDAN': 22.77882797731569, 'DANN': 73.20415879017013}\n",
            "Rare-class stress target acc (%): {'ERM': 22.971221156339123, 'DAN': 17.81177080632616, 'CDAN': 19.471091521908217, 'DANN': 65.67280269639616}\n",
            "Saved shift stress results under /content/drive/MyDrive/DG_PACS/Task1/T1_ShiftStress\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell Q: Consolidated Task-1 summary (table + plot) ===\n",
        "import json, torch, numpy as np, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "\n",
        "def try_summary(path_json, fallback_eval=None, name=None):\n",
        "    if path_json.exists():\n",
        "        j = json.loads(path_json.read_text())\n",
        "        src = j[\"metrics\"].get(\"source_acc\", np.nan)\n",
        "        tgt = j[\"metrics\"].get(\"target_acc\", np.nan)\n",
        "        avg = j[\"metrics\"].get(\"avg_domain_acc\", np.nan)\n",
        "        worst = j[\"metrics\"].get(\"worst_group_acc\", np.nan)\n",
        "        rows.append({\"method\": name, \"source_acc\": src, \"target_acc\": tgt, \"avg\": avg, \"worst\": worst})\n",
        "    elif fallback_eval is not None:\n",
        "        m = fallback_eval()\n",
        "        src,_,_ = evaluate(m, loaders[\"src_test\"], device)\n",
        "        tgt,_,_ = evaluate(m, loaders[\"tgt_test\"], device)\n",
        "        rows.append({\"method\": name, \"source_acc\": float(src), \"target_acc\": float(tgt),\n",
        "                     \"avg\": float((src+tgt)/2), \"worst\": float(min(src,tgt))})\n",
        "\n",
        "# ERM\n",
        "try_summary(SAVE_ROOT/EXP_ERM/\"summary.json\", fallback_eval=build_erm, name=\"ERM\")\n",
        "# DANN\n",
        "try_summary(SAVE_ROOT/EXP_DANN/\"summary_dann.json\", fallback_eval=build_dann_clf, name=\"DANN\")\n",
        "# DAN\n",
        "try_summary(SAVE_ROOT/EXP_DAN/\"summary_dan.json\", fallback_eval=build_dan, name=\"DAN\")\n",
        "# CDAN\n",
        "try_summary(SAVE_ROOT/EXP_CDAN/\"summary_cdan.json\", fallback_eval=build_cdan, name=\"CDAN\")\n",
        "# Self-Training\n",
        "try_summary(SAVE_ROOT/(f\"T1.5_SelfTrain_PACS_{Cfg.SOURCE_DOMAIN}2{Cfg.TARGET_DOMAIN}_ResNet50\")/\"summary_selftrain.json\",\n",
        "            fallback_eval=None, name=\"SelfTrain\")\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(\"target_acc\", ascending=False)\n",
        "OUT_SUMMARY = Path(Cfg.SAVE_ROOT)/\"T1_Summary\"\n",
        "OUT_SUMMARY.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(OUT_SUMMARY/\"task1_summary.csv\", index=False)\n",
        "with open(OUT_SUMMARY/\"task1_summary.json\",\"w\") as f: json.dump(rows, f, indent=2)\n",
        "\n",
        "# Quick bar plot\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(df[\"method\"], df[\"target_acc\"])\n",
        "plt.ylabel(\"Target Accuracy (%)\"); plt.title(\"Task 1: Target Acc by Method\")\n",
        "plt.savefig(OUT_SUMMARY/\"task1_target_acc_bar.png\", bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "\n",
        "print(df)\n",
        "print(f\"Saved consolidated summary to: {OUT_SUMMARY}\")\n"
      ],
      "metadata": {
        "id": "Uyd3DR7v6S8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d229c6-2bce-4067-eef0-b61c771246fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      method  source_acc  target_acc        avg      worst\n",
            "1       DANN  100.000000   65.894630  82.947315  65.894630\n",
            "0        ERM   97.724551   22.550267  60.137409  22.550267\n",
            "4  SelfTrain         NaN   22.092135        NaN        NaN\n",
            "3       CDAN   10.838323   19.292441  15.065382  10.838323\n",
            "2        DAN    8.562874   17.816238  13.189556   8.562874\n",
            "Saved consolidated summary to: /content/drive/MyDrive/DG_PACS/Task1/T1_Summary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell R: Self-Training vs DANN comparison + brief printed analysis ===\n",
        "import json, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "SAVE_ROOT = Path(Cfg.SAVE_ROOT)\n",
        "\n",
        "def _safe_load_json(p):\n",
        "    if Path(p).exists():\n",
        "        with open(p, \"r\") as f: return json.load(f)\n",
        "    return None\n",
        "\n",
        "J_ST   = _safe_load_json(SAVE_ROOT / f\"T1.5_SelfTrain_PACS_{Cfg.SOURCE_DOMAIN}2{Cfg.TARGET_DOMAIN}_ResNet50\" / \"summary_selftrain.json\")\n",
        "J_DANN = _safe_load_json(SAVE_ROOT / \"T1.2_DANN_PACS_photo2sketch_ResNet50_memlite\" / \"summary_dann.json\")\n",
        "\n",
        "def _pull_tgt(j):\n",
        "    if not j: return np.nan\n",
        "    return float(j.get(\"metrics\", {}).get(\"target_acc\", np.nan))\n",
        "\n",
        "tgt_st   = _pull_tgt(J_ST)\n",
        "tgt_dann = _pull_tgt(J_DANN)\n",
        "\n",
        "print(\"=== Self-Training vs DANN (Target Accuracy, %) ===\")\n",
        "print(f\"Self-Training: {tgt_st:.2f}\")\n",
        "print(f\"DANN:         {tgt_dann:.2f}\")\n",
        "if np.isfinite(tgt_st) and np.isfinite(tgt_dann):\n",
        "    diff = tgt_st - tgt_dann\n",
        "    trend = \"higher\" if diff > 0 else (\"lower\" if diff < 0 else \"equal\")\n",
        "    print(f\"Î”(Self-Train âˆ’ DANN) = {diff:+.2f} points ({trend}).\")\n",
        "\n",
        "print(\"\\n--- Analysis (brief) ---\")\n",
        "print(\"Self-training can win when the source-only model already produces moderately reliable pseudo-labels on the target.\")\n",
        "print(\"Even if noisy, the correctly-labeled target samples pull the decision boundary toward target structure.\")\n",
        "print(\"DANN aligns feature distributions globally; if alignment is imperfect or class-conditional alignment is weak,\")\n",
        "print(\"a simple pseudo-label fine-tune (even FC-only) can match or beat it on target accuracy.\")\n",
        "print(\"Pitfalls: confirmation bias (reinforcing wrong pseudo-labels), class imbalance amplification, and threshold choice.\")\n",
        "print(\"Mitigations: higher confidence threshold, class-balanced sampling, temperature/soft labels, or EMA/consistency.\")\n"
      ],
      "metadata": {
        "id": "qhFLkQ9B8pRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d92fc4-b972-4cbf-de92-fbff080749a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Self-Training vs DANN (Target Accuracy, %) ===\n",
            "Self-Training: 22.09\n",
            "DANN:         65.89\n",
            "Î”(Self-Train âˆ’ DANN) = -43.80 points (lower).\n",
            "\n",
            "--- Analysis (brief) ---\n",
            "Self-training can win when the source-only model already produces moderately reliable pseudo-labels on the target.\n",
            "Even if noisy, the correctly-labeled target samples pull the decision boundary toward target structure.\n",
            "DANN aligns feature distributions globally; if alignment is imperfect or class-conditional alignment is weak,\n",
            "a simple pseudo-label fine-tune (even FC-only) can match or beat it on target accuracy.\n",
            "Pitfalls: confirmation bias (reinforcing wrong pseudo-labels), class imbalance amplification, and threshold choice.\n",
            "Mitigations: higher confidence threshold, class-balanced sampling, temperature/soft labels, or EMA/consistency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell S: Label-shift/rare-class visuals (confusions + heatmaps) ===\n",
        "import numpy as np, matplotlib.pyplot as plt, json\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "OUT_SHIFT = Path(Cfg.SAVE_ROOT) / \"T1_ShiftStress\"\n",
        "OUT_SHIFT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Reuse builders from earlier cells (ERM/DAN/DANN/CDAN) ---\n",
        "methods = {\n",
        "    \"ERM\": build_erm,\n",
        "    \"DANN\": lambda: build_dann_clf(),\n",
        "    \"DAN\": build_dan,\n",
        "    \"CDAN\": build_cdan,\n",
        "}\n",
        "\n",
        "# Helper: build a shifted loader and also return its class histogram\n",
        "def build_shifted_loader(base_loader, keep_ratio_by_class):\n",
        "    ds = base_loader.dataset\n",
        "    idx_by_cls = {}\n",
        "    for i in range(len(ds)):\n",
        "        _, y = ds[i]\n",
        "        idx_by_cls.setdefault(int(y), []).append(i)\n",
        "    new_idx = []\n",
        "    for c, idxs in idx_by_cls.items():\n",
        "        k = int(len(idxs) * keep_ratio_by_class.get(c, 1.0))\n",
        "        new_idx.extend(idxs[:max(k,1)])\n",
        "    loader = DataLoader(Subset(ds, new_idx), batch_size=base_loader.batch_size,\n",
        "                        shuffle=False, num_workers=0, pin_memory=False)\n",
        "    # histogram\n",
        "    hist = Counter()\n",
        "    for _, y in loader:\n",
        "        for t in y.tolist(): hist[int(t)] += 1\n",
        "    return loader, hist\n",
        "\n",
        "# Choose label-shift setting (same as Cell P, but weâ€™ll plot now)\n",
        "# pick 3 rare classes programmatically from the unshifted target\n",
        "base_counts = Counter()\n",
        "for _, y in loaders[\"tgt_test\"]:\n",
        "    for v in y.tolist(): base_counts[int(v)] += 1\n",
        "rare3 = [c for c,_ in base_counts.most_common()][-3:]\n",
        "keep_ratio = {c: 0.2 for c in rare3}  # 20% for those classes\n",
        "\n",
        "tgt_shift_loader, shift_counts = build_shifted_loader(loaders[\"tgt_test\"], keep_ratio)\n",
        "\n",
        "# --- distribution heatmap: source vs target vs shifted target ---\n",
        "def counts_to_vec(counter, n):\n",
        "    return np.array([counter.get(i,0) for i in range(n)], dtype=np.float32)\n",
        "\n",
        "# source counts (use your src_test loader as source reference)\n",
        "src_counts = Counter()\n",
        "for _, y in loaders[\"src_test\"]:\n",
        "    for v in y.tolist(): src_counts[int(v)] += 1\n",
        "\n",
        "tgt_counts = base_counts\n",
        "shf_counts = shift_counts\n",
        "\n",
        "mat = np.stack([\n",
        "    counts_to_vec(src_counts, Cfg.NUM_CLASSES),\n",
        "    counts_to_vec(tgt_counts, Cfg.NUM_CLASSES),\n",
        "    counts_to_vec(shf_counts, Cfg.NUM_CLASSES),\n",
        "], axis=0)\n",
        "\n",
        "plt.figure(figsize=(8,3.2))\n",
        "plt.imshow(mat, aspect=\"auto\")\n",
        "plt.yticks([0,1,2], [\"Source\", \"Target\", \"Target (Shifted)\"])\n",
        "plt.xticks(range(Cfg.NUM_CLASSES), [IDX2CLASS[i] for i in range(Cfg.NUM_CLASSES)], rotation=45, ha=\"right\")\n",
        "plt.colorbar(label=\"Count\")\n",
        "plt.title(\"Class Distribution Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_SHIFT / \"class_distribution_heatmap.png\", dpi=160)\n",
        "plt.close()\n",
        "print(f\"Saved: {OUT_SHIFT/'class_distribution_heatmap.png'}\")\n",
        "\n",
        "# --- per-method confusion matrices on shifted target + per-class accuracy heatmap ---\n",
        "def confusion_and_perclass(clf, loader, idx2class):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import torch\n",
        "    cm = np.zeros((len(idx2class), len(idx2class)), dtype=np.int64)\n",
        "    total = correct = 0\n",
        "    true_all, pred_all = [], []\n",
        "    with torch.no_grad():\n",
        "        clf.eval()\n",
        "        for x, y in loader:\n",
        "            x = x.to(device); y = y.to(device)\n",
        "            logits = clf(x); p = logits.argmax(1)\n",
        "            true_all.extend(y.tolist()); pred_all.extend(p.tolist())\n",
        "    cm = confusion_matrix(true_all, pred_all, labels=list(range(len(idx2class))))\n",
        "    per_cls = (cm.diagonal() / np.clip(cm.sum(1), 1, None)) * 100.0\n",
        "    return cm, per_cls\n",
        "\n",
        "percls_mat = []\n",
        "for name, builder in methods.items():\n",
        "    clf = builder().to(device).eval()\n",
        "    # confusion on shifted target:\n",
        "    cm, percls = confusion_and_perclass(clf, tgt_shift_loader, IDX2CLASS)\n",
        "\n",
        "    # save cm figure:\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.imshow(cm, interpolation='nearest')\n",
        "    plt.title(f\"{name}: Confusion (Shifted Target)\"); plt.colorbar()\n",
        "    ticks = np.arange(len(IDX2CLASS))\n",
        "    labels = [IDX2CLASS[i] for i in range(len(IDX2CLASS))]\n",
        "    plt.xticks(ticks, labels, rotation=45, ha='right'); plt.yticks(ticks, labels)\n",
        "    plt.tight_layout(); plt.ylabel('True'); plt.xlabel('Predicted')\n",
        "    fname = OUT_SHIFT / f\"{name.lower()}_cm_shifted.png\"\n",
        "    plt.savefig(fname, bbox_inches=\"tight\", dpi=160); plt.close()\n",
        "    print(f\"Saved: {fname}\")\n",
        "\n",
        "    percls_mat.append(percls)\n",
        "\n",
        "percls_mat = np.vstack(percls_mat)  # [num_methods, num_classes]\n",
        "\n",
        "plt.figure(figsize=(8,3.6))\n",
        "plt.imshow(percls_mat, aspect=\"auto\", vmin=0, vmax=100)\n",
        "plt.yticks(range(len(methods)), list(methods.keys()))\n",
        "plt.xticks(range(Cfg.NUM_CLASSES), [IDX2CLASS[i] for i in range(Cfg.NUM_CLASSES)], rotation=45, ha=\"right\")\n",
        "plt.colorbar(label=\"Per-class Accuracy (%)\")\n",
        "plt.title(\"Per-class Accuracy under Label Shift (Shifted Target)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_SHIFT / \"perclass_accuracy_shift_heatmap.png\", dpi=160)\n",
        "plt.close()\n",
        "print(f\"Saved: {OUT_SHIFT/'perclass_accuracy_shift_heatmap.png'}\")\n"
      ],
      "metadata": {
        "id": "Nh-GG25S8qtk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "e742686b-771e-4578-dfb6-b94d8be161a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'build_erm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3877797618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# --- Reuse builders from earlier cells (ERM/DAN/DANN/CDAN) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m methods = {\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m\"ERM\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_erm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m\"DANN\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_dann_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"DAN\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_dan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_erm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "p_BCCc9egdkU",
        "outputId": "5b95e8d1-5527-4928-b06e-844e3f2e9ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Q1] Unshadowed torchvision.models (removed a dict named 'models').\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'models' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-214372300.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# âš ï¸ Do NOT name this variable `models` (would shadow torchvision.models).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m MODEL_REGISTRY = {\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;34m\"ERM\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_ERM_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;34m\"DAN\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_DAN_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m\"CDAN\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_CDAN_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-214372300.py\u001b[0m in \u001b[0;36mbuild_ERM_clf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# ---- Builders using your saved checkpoints ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_ERM_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet50Classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVE_ROOT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"T1.1_SourceOnly_PACS_photo2sketch_ResNet50\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"ckpts\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"best_by_target.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-873547189.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, pretrained)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet50_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGENET1K_V2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell CS1: Build stress loaders (label-shift & rare-class) and print distributions ===\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "# Helper: downsample classes on an existing loader's dataset\n",
        "def downsample_classes(base_loader, keep_ratio_by_class, max_per_cls=None):\n",
        "    ds = base_loader.dataset\n",
        "    idx_by_cls = {}\n",
        "    for i in range(len(ds)):\n",
        "        _, y = ds[i]\n",
        "        idx_by_cls.setdefault(int(y), []).append(i)\n",
        "    new_idx = []\n",
        "    for c, idxs in idx_by_cls.items():\n",
        "        k = int(len(idxs) * keep_ratio_by_class.get(c, 1.0))\n",
        "        if max_per_cls is not None: k = min(k, max_per_cls)\n",
        "        k = max(k, 1)  # keep at least 1 if class exists\n",
        "        new_idx.extend(idxs[:k])\n",
        "    return DataLoader(\n",
        "        Subset(ds, new_idx),\n",
        "        batch_size=loaders[\"tgt_test\"].batch_size,\n",
        "        shuffle=False, num_workers=0, pin_memory=False\n",
        "    )\n",
        "\n",
        "# 1) Label shift: shrink three classes on target to 20%\n",
        "three = [0, 1, 2]  # you can swap these to other IDs if you prefer\n",
        "keep_ratio_lblshift = {c: 0.2 for c in three}\n",
        "tgt_shift_loader = downsample_classes(loaders[\"tgt_test\"], keep_ratio_lblshift)\n",
        "\n",
        "# 2) Rare-class: keep only 10% of the rarest class on target\n",
        "counts = Counter()\n",
        "for _, yb in loaders[\"tgt_test\"]:\n",
        "    for v in yb.tolist(): counts[int(v)] += 1\n",
        "rarest_cls = min(counts.keys(), key=lambda c: counts[c])\n",
        "tgt_rare_loader = downsample_classes(loaders[\"tgt_test\"], {rarest_cls: 0.1})\n",
        "\n",
        "def count_classes(loader):\n",
        "    c = Counter()\n",
        "    for _, yb in loader:\n",
        "        for v in yb.tolist(): c[int(v)] += 1\n",
        "    return c\n",
        "\n",
        "print(\"[CS1] Target original counts:\", dict(count_classes(loaders[\"tgt_test\"])))\n",
        "print(\"[CS1] Label-shift counts     :\", dict(count_classes(tgt_shift_loader)))\n",
        "print(\"[CS1] Rare-class counts      :\", dict(count_classes(tgt_rare_loader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-DLMqJzbd57",
        "outputId": "6d4575fb-c476-411f-ed4b-863ae236ef35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CS1] Target original counts: {0: 772, 1: 740, 2: 753, 3: 608, 4: 816, 5: 80, 6: 160}\n",
            "[CS1] Label-shift counts     : {0: 154, 1: 148, 2: 150, 3: 608, 4: 816, 5: 80, 6: 160}\n",
            "[CS1] Rare-class counts      : {0: 772, 1: 740, 2: 753, 3: 608, 4: 816, 5: 8, 6: 160}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Patch: (re)bind torchvision.models to the global name `models` ===\n",
        "import torchvision.models as models\n",
        "print(\"[Patch] `models` is now bound to torchvision.models (e.g., resnet50 available).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmuLSixRgDw8",
        "outputId": "181694c3-ca29-4c33-bfd3-0839cc7a7647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch] `models` is now bound to torchvision.models (e.g., resnet50 available).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell CS2: Evaluate under concept-shift loaders with macro stats + plots ===\n",
        "import torchvision.models as models  # keep `models` available for ResNet50Classifier\n",
        "import json, numpy as np, matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as tv_models  # just in case\n",
        "\n",
        "OUT_SHIFT = Path(Cfg.SAVE_ROOT) / \"T1_ShiftStress\"\n",
        "(OUT_SHIFT / \"figs\").mkdir(parents=True, exist_ok=True)\n",
        "device = torch.device(Cfg.DEVICE)\n",
        "\n",
        "# --- Names for classes ---\n",
        "IDX2CLASS = {i: name for i, name in enumerate(label_names)} if 'label_names' in globals() and label_names else {i: f\"class_{i}\" for i in range(Cfg.NUM_CLASSES)}\n",
        "\n",
        "# --- Builders that use your existing checkpoints; DAN/CDAN are optional ---\n",
        "def build_ERM_clf():\n",
        "    m = ResNet50Classifier(num_classes=Cfg.NUM_CLASSES, pretrained=False).to(device)\n",
        "    ck = Path(Cfg.SAVE_ROOT) / \"T1.1_SourceOnly_PACS_photo2sketch_ResNet50\" / \"ckpts\" / \"best_by_target.pt\"\n",
        "    m.load_state_dict(torch.load(ck, map_location=device)); m.eval()\n",
        "    return m\n",
        "\n",
        "def build_DANN_clf_optional():\n",
        "    bb = Path(Cfg.SAVE_ROOT) / \"T1.2_DANN_PACS_photo2sketch_ResNet50_memlite\" / \"ckpts\" / \"best_dann_backbone.pt\"\n",
        "    if not bb.exists(): return None\n",
        "    feat = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=False).to(device)\n",
        "    feat.load_state_dict(torch.load(bb, map_location=device)); feat.eval()\n",
        "    class Wrap(nn.Module):\n",
        "        def __init__(self, feat): super().__init__(); self.backbone=feat\n",
        "        def forward(self, x):\n",
        "            f = self.backbone(x, return_feat=False, class_head=False)\n",
        "            return self.backbone.classifier(f)\n",
        "    return Wrap(feat).to(device).eval()\n",
        "\n",
        "def build_DAN_clf_optional():\n",
        "    bdir = Path(Cfg.SAVE_ROOT) / \"T1.3_DAN_PACS_photo2sketch_ResNet50_memlite\" / \"ckpts\"\n",
        "    if not (bdir / \"best_dan_backbone.pt\").exists() or not (bdir / \"best_dan_head.pt\").exists():\n",
        "        return None\n",
        "    feat = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=False).to(device)\n",
        "    feat.load_state_dict(torch.load(bdir/\"best_dan_backbone.pt\", map_location=device)); feat.eval()\n",
        "    class DAN_Head(nn.Module):\n",
        "        def __init__(self, in_dim, num_classes, hid=256, p=0.2):\n",
        "            super().__init__()\n",
        "            self.bottleneck = nn.Sequential(\n",
        "                nn.Linear(in_dim, hid), nn.BatchNorm1d(hid), nn.ReLU(inplace=True), nn.Dropout(p),\n",
        "            )\n",
        "            self.classifier = nn.Linear(hid, num_classes)\n",
        "        def forward(self, f): z = self.bottleneck(f); return self.classifier(z)\n",
        "    head = DAN_Head(feat.feat_dim, Cfg.NUM_CLASSES).to(device)\n",
        "    head.load_state_dict(torch.load(bdir/\"best_dan_head.pt\", map_location=device)); head.eval()\n",
        "    class Wrap(nn.Module):\n",
        "        def __init__(self, feat, head): super().__init__(); self.backbone=feat; self.head=head\n",
        "        def forward(self, x):\n",
        "            f = self.backbone(x, return_feat=False, class_head=False)\n",
        "            return self.head(f)\n",
        "    return Wrap(feat, head).to(device).eval()\n",
        "\n",
        "def build_CDAN_clf_optional():\n",
        "    cdir = Path(Cfg.SAVE_ROOT) / f\"T1.4_CDAN_PACS_{Cfg.SOURCE_DOMAIN}2{Cfg.TARGET_DOMAIN}_ResNet50_memlite\" / \"ckpts\"\n",
        "    if not (cdir / \"best_cdan_backbone.pt\").exists() or not (cdir / \"best_cdan_head.pt\").exists():\n",
        "        return None\n",
        "    feat = ResNet_Feature(num_classes=Cfg.NUM_CLASSES, backbone=\"resnet50\", pretrained=False).to(device)\n",
        "    feat.load_state_dict(torch.load(cdir/\"best_cdan_backbone.pt\", map_location=device)); feat.eval()\n",
        "    class CDAN_Head(nn.Module):\n",
        "        def __init__(self, in_dim, num_classes, hid=256, p=0.2):\n",
        "            super().__init__()\n",
        "            self.bottleneck = nn.Sequential(\n",
        "                nn.Linear(in_dim, hid), nn.BatchNorm1d(hid), nn.ReLU(inplace=True), nn.Dropout(p),\n",
        "            )\n",
        "            self.classifier = nn.Linear(hid, num_classes)\n",
        "        def forward(self, f): z = self.bottleneck(f); return self.classifier(z)\n",
        "    head = CDAN_Head(feat.feat_dim, Cfg.NUM_CLASSES).to(device)\n",
        "    head.load_state_dict(torch.load(cdir/\"best_cdan_head.pt\", map_location=device)); head.eval()\n",
        "    class Wrap(nn.Module):\n",
        "        def __init__(self, feat, head): super().__init__(); self.backbone=feat; self.head=head\n",
        "        def forward(self, x):\n",
        "            f = self.backbone(x, return_feat=False, class_head=False)\n",
        "            return self.head(f)\n",
        "    return Wrap(feat, head).to(device).eval()\n",
        "\n",
        "# Registry (conditional for DAN/CDAN)\n",
        "MODEL_REG = {\"ERM\": build_ERM_clf()}\n",
        "maybe = build_DANN_clf_optional()\n",
        "if maybe is not None: MODEL_REG[\"DANN\"] = maybe\n",
        "maybe = build_DAN_clf_optional()\n",
        "if maybe is not None: MODEL_REG[\"DAN\"] = maybe\n",
        "maybe = build_CDAN_clf_optional()\n",
        "if maybe is not None: MODEL_REG[\"CDAN\"] = maybe\n",
        "\n",
        "print(\"[CS2] Evaluating methods:\", list(MODEL_REG.keys()))\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_full(model, loader, num_classes):\n",
        "    model.eval()\n",
        "    ys, yh = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = logits.argmax(1)\n",
        "        ys.append(yb.cpu().numpy()); yh.append(preds.cpu().numpy())\n",
        "    ys = np.concatenate(ys) if ys else np.zeros(0, dtype=int)\n",
        "    yh = np.concatenate(yh) if yh else np.zeros(0, dtype=int)\n",
        "\n",
        "    acc = 100.0 * (yh == ys).mean() if ys.size else 0.0\n",
        "    cm = confusion_matrix(ys, yh, labels=list(range(num_classes)))\n",
        "    per_cls = {IDX2CLASS[i]: (100.0 * cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0.0)\n",
        "               for i in range(num_classes)}\n",
        "    # Macro metrics\n",
        "    per_acc = [(cm[i, i] / cm[i].sum()) if cm[i].sum() > 0 else 0.0 for i in range(num_classes)]\n",
        "    macro_acc = 100.0 * float(np.mean(per_acc)) if per_acc else 0.0\n",
        "    macro_f1  = 100.0 * f1_score(ys, yh, average=\"macro\", labels=list(range(num_classes)), zero_division=0)\n",
        "    return acc, macro_acc, macro_f1, per_cls, cm\n",
        "\n",
        "def plot_cm(cm, title, savepath):\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.imshow(cm, interpolation='nearest')\n",
        "    plt.title(title); plt.colorbar()\n",
        "    ticks = np.arange(Cfg.NUM_CLASSES)\n",
        "    plt.xticks(ticks, [IDX2CLASS[i] for i in ticks], rotation=45, ha='right')\n",
        "    plt.yticks(ticks, [IDX2CLASS[i] for i in ticks])\n",
        "    plt.ylabel(\"True\"); plt.xlabel(\"Predicted\")\n",
        "    plt.tight_layout(); plt.savefig(savepath, dpi=160); plt.close()\n",
        "\n",
        "def class_hist(loader):\n",
        "    c = np.zeros(Cfg.NUM_CLASSES, dtype=int)\n",
        "    for _, yb in loader:\n",
        "        for v in yb.tolist(): c[int(v)] += 1\n",
        "    return c\n",
        "\n",
        "def eval_pack(tag, loader):\n",
        "    pack = {}\n",
        "    for name, m in MODEL_REG.items():\n",
        "        acc, macro_acc, macro_f1, per_cls, cm = eval_full(m, loader, Cfg.NUM_CLASSES)\n",
        "        pack[name] = {\n",
        "            \"acc_overall\": float(acc),\n",
        "            \"macro_acc\": float(macro_acc),\n",
        "            \"macro_f1\": float(macro_f1),\n",
        "            \"per_class_acc\": {k: float(v) for k, v in per_cls.items()},\n",
        "        }\n",
        "        plot_cm(cm, f\"{name} â€” {tag}\", OUT_SHIFT / \"figs\" / f\"cm_{tag}_{name}.png\")\n",
        "    with open(OUT_SHIFT / f\"{tag}_macro_metrics.json\", \"w\") as f:\n",
        "        json.dump(pack, f, indent=2)\n",
        "    print(f\"[{tag}] saved macro/per-class metrics and confusion matrices.\")\n",
        "\n",
        "# Run on your *shift* loaders built in CS1\n",
        "eval_pack(\"label_shift\", tgt_shift_loader)\n",
        "eval_pack(\"rare_stress\", tgt_rare_loader)\n",
        "\n",
        "# Heatmap: Source vs. label-shift target class counts\n",
        "src_counts = class_hist(loaders[\"src_test\"])\n",
        "shift_counts = class_hist(tgt_shift_loader)\n",
        "plt.figure(figsize=(6.6,3.6))\n",
        "plt.imshow(np.vstack([src_counts, shift_counts]), aspect=\"auto\")\n",
        "plt.yticks([0,1], [\"Source\", \"Target (label-shift)\"])\n",
        "plt.xticks(range(Cfg.NUM_CLASSES), [IDX2CLASS[i] for i in range(Cfg.NUM_CLASSES)], rotation=45, ha='right')\n",
        "plt.title(\"Class distribution heatmap â€” Source vs Target (label-shift)\")\n",
        "plt.colorbar(label=\"count\"); plt.tight_layout()\n",
        "plt.savefig(OUT_SHIFT / \"figs\" / \"label_shift_heatmap.png\", dpi=160); plt.close()\n",
        "print(f\"[CS2] Saved heatmap -> {OUT_SHIFT/'figs'/'label_shift_heatmap.png'}\")\n"
      ],
      "metadata": {
        "id": "ifraawVmgg2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607e384a-129b-49ce-d700-d5d8be27f295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CS2] Evaluating methods: ['ERM', 'DANN', 'DAN', 'CDAN']\n",
            "[label_shift] saved macro/per-class metrics and confusion matrices.\n",
            "[rare_stress] saved macro/per-class metrics and confusion matrices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CS2] Saved heatmap -> /content/drive/MyDrive/DG_PACS/Task1/T1_ShiftStress/figs/label_shift_heatmap.png\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}